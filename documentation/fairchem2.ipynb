{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a3bdf6",
   "metadata": {},
   "source": [
    "# FairChem v2 with Docker for Local Development and Cloud Scaling\n",
    "\n",
    "---\n",
    "\n",
    "This notebook introduces **FairChem v2**, a lightweight and production-friendly cheminformatics framework designed for scalable property prediction and molecule representation learning. This walkthrough covers:\n",
    "\n",
    "1. **Introduction to FairChem v2**: Historical context and motivations.\n",
    "2. **Setting Up the Development Environment**: Docker-based instructions.\n",
    "3. **Building and Running the Docker Container**: Easily containerize FairChem v2.\n",
    "4. **Getting Started with Predictions**: Quick CLI and API usage.\n",
    "5. **Extending to Cloud**: How to scale FairChem v2 using cloud infrastructure.\n",
    "\n",
    "## Introduction to FairChem v2 and the Evolution of Molecular Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "The quest to computationally predict molecular properties began with quantum chemistry methods like **Hartree–Fock** and **Density Functional Theory (DFT)**—powerful but computationally expensive techniques that formed the backbone of early molecular modeling.\n",
    "\n",
    "In the 2000s, the field shifted toward **machine learning models** like kernel ridge regression and random forests trained on hand-crafted descriptors (e.g., MACCS, ECFP), enabling fast screening workflows.\n",
    "\n",
    "A paradigm shift occurred in the 2010s with the rise of **graph neural networks (GNNs)**, treating molecules as graphs with atoms as nodes and bonds as edges. Pioneering models such as **Message Passing Neural Networks (MPNNs)**, **WeaveNet**, and **GraphConv** learned molecular representations end-to-end.\n",
    "\n",
    "By 2020, architectures like [SchNet](#refs‑sch-net), [DimeNet](#refs‑dimenet), and [PhysNet](#refs-psynet) incorporated geometric and quantum structure, achieving higher generalizability across different molecular tasks. Self-supervised pretraining (e.g., contrastive learning, masking) became common by 2023, fostering **foundation models for chemistry**.\n",
    "\n",
    "**FairChem v2** builds on this rich legacy:\n",
    "\n",
    "- Packs pretrained GNNs and Transformer-style architectures into one cohesive `fairchem-core` package (no need for PyG, torch-scatter, etc.) [[10]](#refs‑install)\n",
    "- Offers both **2D and 3D input support**, enabling versatile use cases\n",
    "- Provides CLI and Python API for flexible workflows\n",
    "- Designed for scalable deployment—locally via Docker and on the cloud\n",
    "\n",
    "Now maintained under Meta FAIR Chemistry (formerly the Open Catalyst Project) [[4]](#refs‑meta), FairChem v2 is production-ready, with models like UMA and OMAT integrated into ASE and Hugging Face pipelines.\n",
    "\n",
    "Whether in drug discovery, catalysis, materials science, or reaction optimization, FairChem v2 provides a comprehensive, efficient platform for modern molecular machine learning.\n",
    "\n",
    "\n",
    "## Notebook Roadmap\n",
    "\n",
    "---\n",
    "\n",
    "### Sections\n",
    "- [Building and Running the Docker Container](#building-and-running-the-docker-container)\n",
    "- [Using FairChem v2](#using-fairchem-v2)\n",
    "- [Running a Property Prediction Example](#property-prediction-example)\n",
    "- [Deploying to the Cloud](#deploying-to-the-cloud)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "To follow this tutorial, ensure the following are installed on your system:\n",
    "\n",
    "- [Docker](https://docs.docker.com/get-docker/)\n",
    "- A GPU-compatible system (recommended)\n",
    "- NVIDIA Container Toolkit if using GPU\n",
    "- [VSCode](https://code.visualstudio.com/) (optional for container access)\n",
    "\n",
    "\n",
    "## Building and Running the Docker Container\n",
    "\n",
    "---\n",
    "\n",
    "1. **Clone the FairChem v2 Repository**\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/gabenavarro/MLContainerLab.git\n",
    "cd MLContainerLab\n",
    "```\n",
    "\n",
    "2. **Build the Docker Image**: Use the provided Dockerfile to build the Docker image.\n",
    "\n",
    "Before you build your docker image, make sure you have access to [UMA model repository](#refs-uma-models) created a [huggingface-cli access token](#ref-huggingface-cli-token). Save the token string into to gitingore file `./assets/secrets/huggingface.token`. This will make sure the dockerfile places huggingface access token in appropriate path. Alternatively, run `huggingface-cli login` in running container to add token later.\n",
    "\n",
    "```bash\n",
    "# You can choose any tag you want for the image\n",
    "# Feel free to play around with the base image, just make sure the host has the same or higher CUDA version\n",
    "docker build -f ./assets/build/Dockerfile.fairchem2.cu126cp310 -t fairchem2:126-310 .\n",
    "```\n",
    "3. **Run the Docker Container**: Run the Docker container with the necessary configurations. In the first example, we will run the container locally with GPU support. This is the recommended way to run a container while in development mode. For scaling up, we will use the second example which runs the container in the cloud.\n",
    "\n",
    "```bash\n",
    "# Run the container with GPU support\n",
    "docker run -dt \\\n",
    "   --gpus all \\\n",
    "   --shm-size=64g \\\n",
    "   -v \"$(pwd):/workspace\" \\\n",
    "   --name fairchem2 \\\n",
    "   --env NVIDIA_VISIBLE_DEVICES=all \\\n",
    "   --env GOOGLE_APPLICATION_CREDENTIALS=/workspace/assets/secrets/gcp-key.json \\\n",
    "   fairchem2:126-310\n",
    "```\n",
    "> Note: The `-v \"$(pwd):/workspace\"` option mounts the current directory to `/workspace` in the container, allowing you to access your local files from within the container. The `--env` options set environment variables for GPU visibility and Google Cloud credentials.<br>\n",
    "> Note: The `--gpus all` option allows the container to use all available GPUs. <br>\n",
    "\n",
    "4. **Access the Container with IDE**: In this example, we will use Visual Studio Code to access the container. You can use any IDE of your choice.\n",
    "\n",
    "```bash\n",
    "# In a scriptable manner\n",
    "CONTAINER_NAME=fairchem2\n",
    "FOLDER=/workspace\n",
    "HEX_CONFIG=$(printf {\\\"containerName\\\":\\\"/$CONTAINER_NAME\\\"} | od -A n -t x1 | tr -d '[\\n\\t ]')\n",
    "code --folder-uri \"vscode-remote://attached-container+$HEX_CONFIG$FOLDER\"\n",
    "```\n",
    "\n",
    "> Note: The `code` command is used to open Visual Studio Code. Make sure you have the Remote - Containers extension installed in VS Code to access the container directly. <br>\n",
    "> Note: Make sure you have installed Remote - Containers extension in VS Code.<br>\n",
    "\n",
    "\n",
    "[#ref-huggingface-cli-token]: https://huggingface.co/settings/tokens \"Access tokens authenticate your identity to the Hugging Face Hub and allow applications to perform actions based on token permissions.\"\n",
    "[#refs-psynet]: https://github.com/MMunibas/PhysNet \"PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments and Partial Charges\"\n",
    "[#refs-uma-models]: https://huggingface.co/facebook/UMA \"UMA model agreement\"\n",
    "[#refs‑sch-net]: https://arxiv.org/abs/1712.06113 \"SchNet: a deep learning architecture for molecules and materials\"\n",
    "[#refs‑dimenet]: https://arxiv.org/abs/2003.03123 \"DimeNet: Directional Message Passing Neural Network\"\n",
    "[#refs‑install]: https://fair-chem.github.io/core/install.html \"FairChem‑core installation notes\"\n",
    "[#refs‑meta]: https://github.com/FAIRChem \"FAIR Chemistry @ Meta (was Open Catalyst Project)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335d277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import units\n",
    "from ase.io import Trajectory\n",
    "from ase.md.langevin import Langevin\n",
    "from ase.build import molecule\n",
    "from fairchem.core import pretrained_mlip, FAIRChemCalculator\n",
    "from fairchem.core.datasets import atomic_data\n",
    "\n",
    "predictor = pretrained_mlip.get_predict_unit(\"uma-s-1\", device=\"cuda\")\n",
    "calc = FAIRChemCalculator(predictor, task_name=\"omol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4efb83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from fairchem.core import FAIRChemCalculator, pretrained_mlip\n",
    "from ase.build import bulk, molecule\n",
    "from fairchem.core.datasets.atomic_data import AtomicData, atomicdata_list_to_batch\n",
    "from fairchem.core.datasets.embeddings import khot_embeddings\n",
    "from ase.atoms import Atoms\n",
    "\n",
    "predictor = pretrained_mlip.get_predict_unit(\"uma-s-1\", device=\"cuda\")\n",
    "calc = FAIRChemCalculator(predictor, task_name=\"omol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ec90573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy': tensor([-2079.4659], device='cuda:0', dtype=torch.float64,\n",
       "        grad_fn=<CatBackward0>),\n",
       " 'forces': tensor([[ 3.2495e-05,  4.0628e-05, -9.3457e-01],\n",
       "         [-1.8890e-03, -2.5103e-01,  4.6724e-01],\n",
       "         [ 1.8565e-03,  2.5099e-01,  4.6732e-01]], device='cuda:0'),\n",
       " 'stress': tensor([[ 0.0000e+00,  1.4294e-03, -9.6884e-06,  1.4294e-03,  3.8317e-01,\n",
       "           1.8400e-05, -9.6884e-06,  1.8400e-05,  5.5729e-01]], device='cuda:0',\n",
       "        grad_fn=<CatBackward0>)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "atomic_data_list = [\n",
    "    AtomicData.from_ase(molecule(\"H2O\"), task_name=\"omol\"),\n",
    "    # AtomicData.from_ase(molecule(\"C7NH5\"), task_name=\"omol\").to(\"cuda\")\n",
    "]\n",
    "batch = atomicdata_list_to_batch(atomic_data_list)\n",
    "predictor.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b4e1a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oc20_energy': {'energy': tensor([0.], device='cuda:0')},\n",
       " 'oc20_forces': {'forces': tensor([[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]], device='cuda:0')},\n",
       " 'oc20_stress': {'stress': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')},\n",
       " 'odac_energy': {'energy': tensor([0.], device='cuda:0')},\n",
       " 'odac_forces': {'forces': tensor([[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]], device='cuda:0')},\n",
       " 'odac_stress': {'stress': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')},\n",
       " 'omat_energy': {'energy': tensor([0.], device='cuda:0')},\n",
       " 'omat_forces': {'forces': tensor([[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]], device='cuda:0')},\n",
       " 'omat_stress': {'stress': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')},\n",
       " 'omc_energy': {'energy': tensor([0.], device='cuda:0')},\n",
       " 'omc_forces': {'forces': tensor([[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]], device='cuda:0')},\n",
       " 'omc_stress': {'stress': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')},\n",
       " 'omol_energy': {'energy': tensor([0.1649], device='cuda:0', grad_fn=<IndexPutBackward0>)},\n",
       " 'omol_forces': {'forces': tensor([[ 2.0023e-08, -0.0000e+00, -6.5675e-01],\n",
       "          [-1.3158e-03, -1.7669e-01,  3.2838e-01],\n",
       "          [ 1.3158e-03,  1.7669e-01,  3.2838e-01]], device='cuda:0')},\n",
       " 'omol_stress': {'stress': tensor([[ 0.0000e+00,  1.0043e-03, -5.9663e-09,  1.0043e-03,  2.6971e-01,\n",
       "            0.0000e+00, -5.9663e-09,  0.0000e+00,  3.9163e-01]], device='cuda:0',\n",
       "         grad_fn=<IndexPutBackward0>)}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cae43d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function copyreg._reconstructor(cls, base, state)>,\n",
       " (fairchem.core.units.mlip_unit.api.inference.InferenceSettings, object, None),\n",
       " {'tf32': True,\n",
       "  'activation_checkpointing': True,\n",
       "  'merge_mole': True,\n",
       "  'compile': False,\n",
       "  'wigner_cuda': True,\n",
       "  'external_graph_gen': False,\n",
       "  'internal_graph_gen_version': 2})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.inference_mode.__reduce__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a8300b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numbers': array([8, 1, 1]),\n",
       " 'positions': array([[ 0.      ,  0.      ,  0.119262],\n",
       "        [ 0.      ,  0.763239, -0.477047],\n",
       "        [ 0.      , -0.763239, -0.477047]]),\n",
       " 'cell': array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " 'pbc': array([False, False, False])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = pretrained_mlip.get_predict_unit(\"uma-s-1\", device=\"cuda\")\n",
    "calc = FAIRChemCalculator(predictor, task_name=\"omol\")\n",
    "\n",
    "atoms = molecule(\"H2O\")\n",
    "atoms.calc = calc\n",
    "\n",
    "atoms.todict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d194f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
