{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbc9458",
   "metadata": {},
   "source": [
    "# Mamba State Space Model with Docker for Local Development and Cloud Deployment\n",
    "\n",
    "This documentation provides a guide on Mamba State Space Model (SSM) implemented in Python, designed for both local development and cloud deployment using Docker. It covers the following topics:\n",
    "\n",
    "1. **Introduction to Mamba SSM**: Overview of the Mamba State Space Model and its applications.\n",
    "2. **Setting Up the Development Environment**: Step-by-step instructions for setting up a local development environment using Docker.\n",
    "3. **Building and Running the Docker Container**: Instructions for building the Docker image and running the container.\n",
    "4. **Deploying to the Cloud**: Guidelines for deploying the Mamba SSM to a cloud platform using Docker.\n",
    "5. **Best Practices**: Tips and best practices for working with Mamba SSM and Docker.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following installed on your local machine:\n",
    "\n",
    "- Docker: [Install Docker](https://docs.docker.com/get-docker/)\n",
    "- A compatible GPU (for Mamba SSM)\n",
    "- NVIDIA drivers (if using GPU)\n",
    "\n",
    "\n",
    "## Sections\n",
    "- [Introduction to Mamba SSM](#introduction-to-mamba-ssm)\n",
    "- [Building and Running the Docker Container](#building-and-running-the-docker-container)\n",
    "- [Using Mamba SSM](#using-mamba-ssm)\n",
    "- [Deploying to the Cloud](#deploying-to-the-cloud)\n",
    "\n",
    "\n",
    "## Introduction to Mamba SSM\n",
    "\n",
    "At the heart of modern AI systems like ChatGPT and AlphaFold lies a critical challenge: understanding sequences. Whether you're parsing a sentence, listening to speech, decoding a genome, or analyzing financial time series, the task boils down to learning patterns across time or space. For the past few years, Transformers—powered by the self-attention mechanism—have dominated this space due to their remarkable ability to model complex relationships between every pair of inputs. But this power comes at a cost: Transformers scale poorly with longer sequences and become expensive or infeasible beyond a few thousand steps.\n",
    "\n",
    "Enter Mamba, a new kind of sequence model inspired by state space systems—an elegant mathematical framework traditionally used in physics and control theory. Think of it like replacing a massive all-to-all attention grid with a sleek, memory-efficient signal processor that knows when to pay attention and when to ignore noise.\n",
    "\n",
    "What makes Mamba different isn’t just that it’s faster (though it is—often 3× faster than its peers on modern GPUs), or that it can handle million-token contexts (which it can, with ease). It’s that Mamba is selective. Unlike older state space models that process information uniformly over time, Mamba can decide what matters at each step. It brings a kind of intelligent filtering—like memory with a spotlight—selectively storing important details and discarding the rest.\n",
    "\n",
    "Here’s the kicker: despite being fully recurrent and operating in linear time, Mamba matches or exceeds the accuracy of Transformers in domains ranging from text and audio to genomics. It doesn’t need attention layers or even separate MLP blocks. Its design is minimal, clean, and hardware-aware—making it not only smart, but fast.\n",
    "\n",
    "By blending the long-term memory of RNNs, the locality of CNNs, and the expressive power of Transformers—all within a scalable, streamlined architecture—Mamba represents a profound shift in how we think about modeling sequences at scale.\n",
    "\n",
    "\n",
    "## Building and Running the Docker Container\n",
    "\n",
    "To build and run the Docker container for Mamba SSM, follow these steps:\n",
    "\n",
    "1. **Clone the Repository**: Clone the Mamba SSM repository to your local machine.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/gabenavarro/MLContainerLab.git\n",
    "   cd MLContainerLab\n",
    "   ```\n",
    "\n",
    "2. **Build the Docker Image**: Use the provided Dockerfile to build the Docker image.\n",
    "\n",
    "   ```bash\n",
    "   # You can choose any tag you want for the image\n",
    "   # Feel free to play around with the base image, just make sure the host has the same or higher CUDA version\n",
    "   docker build -f ./assets/build/Dockerfile.mamba.cu128py26cp312 -t ssm-mamba:128-26-312 .\n",
    "   ```\n",
    "3. **Run the Docker Container**: Run the Docker container with the necessary configurations. In the first example, we will run the container locally with GPU support. This is the recommended way to run a container while in development mode. For scaling up, we will use the second example which runs the container in the cloud.\n",
    "\n",
    "   ```bash\n",
    "    # Run the container with GPU support\n",
    "    docker run -dt \\\n",
    "        --gpus all \\\n",
    "        -v \"$(pwd):/workspace\" \\\n",
    "        --name ssm-mamba \\\n",
    "        --env NVIDIA_VISIBLE_DEVICES=all \\\n",
    "        --env GOOGLE_APPLICATION_CREDENTIALS=/workspace/assets/secrets/gcp-key.json \\\n",
    "        ssm-mamba:128-26-312\n",
    "    ```\n",
    "> Note: The `-v \"$(pwd):/workspace\"` option mounts the current directory to `/workspace` in the container, allowing you to access your local files from within the container. The `--env` options set environment variables for GPU visibility and Google Cloud credentials.<br>\n",
    "> Note: The `--gpus all` option allows the container to use all available GPUs. <br>\n",
    "\n",
    "4. **Access the Container with IDE**: In this example, we will use Visual Studio Code to access the container. You can use any IDE of your choice.\n",
    "\n",
    "   ```bash\n",
    "   # In a scriptable manner\n",
    "   CONTAINER_NAME=ssm-mamba\n",
    "   FOLDER=/workspace\n",
    "   HEX_CONFIG=$(printf {\\\"containerName\\\":\\\"/$CONTAINER_NAME\\\"} | od -A n -t x1 | tr -d '[\\n\\t ]')\n",
    "   code --folder-uri \"vscode-remote://attached-container+$HEX_CONFIG$FOLDER\"\n",
    "   ```\n",
    "\n",
    "> Note: The `code` command is used to open Visual Studio Code. Make sure you have the Remote - Containers extension installed in VS Code to access the container directly. <br>\n",
    "> Note: Make sure you have installed Remote - Containers extension in VS Code.<br>\n",
    "\n",
    "## Using Mamba SSM\n",
    "\n",
    "We will now train a simple Mamba SSM model below using the installed dockerized mamba package above. We will be using a training routine that is similar to training language models with a dataset of bitcoin prices. It will have severe overfitting, but it will be enough to show how to use the package. The training routine is similar to the one used in the [Mamba SSM repository](https://github.com/gabenavarro/MLContainerLab/tree/main/assets/examples/mamba).\n",
    "\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "First, lets go ahead and download the data. We will use a limited dataset from Kaggle to start, however in more advanced scenarios its highly suggested to use an API with access to more datasets such as the [CoinGecko API](https://www.coingecko.com/en/api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a138284",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download and unzip the Bitcoin historical data dataset from Kaggle\n",
    "!curl -L -o /workspace/datasets/bitcoin-historical-data.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/mczielinski/bitcoin-historical-data \\\n",
    "    && unzip -o /workspace/datasets/bitcoin-historical-data.zip -d /workspace/datasets/ \\\n",
    "    && rm /workspace/datasets/bitcoin-historical-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SERIES_CSV = \"/workspace/datasets/btcusd_1-min_data.csv\"\n",
    "PROCESSED_DATA_DIR = \"/workspace/datasets/auto_regressive_processed_timeseries\"\n",
    "CKPT_DIR = \"/workspace/datasets/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories if they do not exist\n",
    "import os\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcaabdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.read_csv(TIME_SERIES_CSV, low_memory=False, nrows=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11da070",
   "metadata": {},
   "source": [
    "Now that we have the dataset, lets create a dataset and dataloader for the model using litdata. Litdata is a lightweight data loading library that is designed to work with PyTorch and other deep learning frameworks. It provides a simple and efficient way to load and preprocess data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8683470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import litdata as ld\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "def process_timeseries(file_path: str, sequence_length: int = 2048) -> Dict[str, Any]:\n",
    "    \"\"\"Process a timeseries CSV file into a format suitable for autoregressive modeling.\"\"\"\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    if \"datetime\" in df.columns:\n",
    "        df = df.drop(columns=[\"datetime\"])\n",
    "    df = df.sort_values('Timestamp')\n",
    "    numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # —— NEW: log-transform price values to handle exponential growth ——\n",
    "    for price_col in ['Open', 'High', 'Low', 'Close']:\n",
    "        # Ensure all values are positive before log transform\n",
    "        df[price_col] = np.log(df[price_col].replace([np.inf, -np.inf, 0, np.nan], 0.01).fillna(0.01))\n",
    "    \n",
    "    # —— NEW: log1p-transform volume to reduce extreme skew ——\n",
    "    df['Volume'] = np.log1p(df['Volume'].replace([np.inf, -np.inf], np.nan).fillna(0.0))\n",
    "    \n",
    "    stats = {}\n",
    "    for feature in numerical_features:\n",
    "        vals = df[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        mean, std = (vals.mean(), vals.std() or 1.0)\n",
    "        stats[feature] = {'mean': mean, 'std': std}\n",
    "        df[feature] = (df[feature] - mean) / std\n",
    "\n",
    "    if len(df) <= sequence_length:\n",
    "        print(f\"Warning: only {len(df)} rows < sequence_length={sequence_length}\")\n",
    "        return None\n",
    "\n",
    "    def create_timeseries_sample(index: int) -> Dict[str, Any]:\n",
    "        if index < sequence_length or index >= len(df):\n",
    "            return {\"index\": index,\n",
    "                    \"inputs\": np.zeros((sequence_length,5),dtype=np.float32),\n",
    "                    \"mask\": np.zeros(sequence_length, dtype=bool),\n",
    "                    \"stats\": stats}\n",
    "        seq = df.iloc[index-sequence_length:index][numerical_features].values\n",
    "        arr = np.nan_to_num(seq.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mask = ~np.isnan(seq).any(axis=1)\n",
    "        return {\"index\": index, \"inputs\": arr, \"mask\": mask, \"stats\": stats}\n",
    "\n",
    "    # Store original stats for inverse transformation during inference\n",
    "    stats['transform_type'] = 'log_then_zscore'\n",
    "    \n",
    "    return create_timeseries_sample\n",
    "\n",
    "\n",
    "# Set the sequence length for your model\n",
    "sequence_length = 2048\n",
    "\n",
    "# Get the processing function configured for your specific file\n",
    "process_function = process_timeseries(TIME_SERIES_CSV, sequence_length)\n",
    "\n",
    "# Filter indices to exclude those with NaN values\n",
    "def get_valid_indices():\n",
    "    df = pd.read_csv(TIME_SERIES_CSV, low_memory=False)\n",
    "    if \"datetime\" in df.columns:\n",
    "        df = df.drop(columns=[\"datetime\"])\n",
    "    \n",
    "    df = df.sort_values('Timestamp')\n",
    "    numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Replace infinity values with NaN\n",
    "    for feature in numerical_features:\n",
    "        df[feature] = df[feature].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    valid_indices = []\n",
    "    for idx in range(sequence_length, len(df), int(sequence_length * 0.25)):\n",
    "        # Check if the entire sequence has no NaN values\n",
    "        sequence = df.iloc[idx-sequence_length:idx][numerical_features].values\n",
    "        if not np.isnan(sequence).any() and not np.isinf(sequence).any():\n",
    "            valid_indices.append(idx)\n",
    "        else:\n",
    "            print(f\"Skipping index {idx} due to NaN or inf values in the sequence.\")\n",
    "    \n",
    "    if not valid_indices:\n",
    "        raise ValueError(\"No valid sequences found! All sequences contain NaN or inf values.\")\n",
    "    \n",
    "    return valid_indices\n",
    "\n",
    "valid_indices = get_valid_indices()\n",
    "\n",
    "# The optimize function writes data in an optimized format\n",
    "ld.optimize(\n",
    "    fn=process_function,              # the function that processes each sample\n",
    "    inputs=valid_indices,             # the indices of valid samples\n",
    "    output_dir=PROCESSED_DATA_DIR,    # optimized data is stored here\n",
    "    num_workers=4,                    # The number of workers on the same machine\n",
    "    chunk_bytes=\"64MB\"                # size of each chunk\n",
    ")\n",
    "# Takes about 30 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79526ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litdata import StreamingDataset, StreamingDataLoader, train_test_split\n",
    "import torch\n",
    "streaming_dataset = StreamingDataset(PROCESSED_DATA_DIR) # data are stored in the cloud\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        # Return empty tensors if the batch is empty\n",
    "        return {\n",
    "            \"index\": torch.tensor([], dtype=torch.long),\n",
    "            \"inputs\": torch.tensor([], dtype=torch.float32),\n",
    "            \"mask\": torch.tensor([], dtype=torch.bool),\n",
    "            \"stats\": {}\n",
    "        }\n",
    "    \n",
    "    # Process each key separately\n",
    "    indices = torch.tensor([item[\"index\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # Make sure arrays are writable by copying and convert to tensor\n",
    "    inputs = torch.stack([torch.tensor(np.nan_to_num(item[\"inputs\"].copy(), nan=0.0), dtype=torch.float32) for item in batch])\n",
    "    masks = torch.stack([torch.tensor(item[\"mask\"].copy(), dtype=torch.bool) for item in batch])\n",
    "    \n",
    "    # Get stats (use first non-empty item's stats)\n",
    "    stats = next((item[\"stats\"] for item in batch if \"stats\" in item), {})\n",
    "    \n",
    "    return {\n",
    "        \"index\": indices,\n",
    "        \"inputs\": inputs,\n",
    "        \"mask\": masks,\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "print(len(streaming_dataset)) # display the length of your data\n",
    "# out: 100,000\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = train_test_split(streaming_dataset, splits=[0.8, 0.1, 0.1])\n",
    "\n",
    "print(\"Train \", len(train_dataset))\n",
    "train_dataloader = StreamingDataLoader(train_dataset, num_workers=4, batch_size=32, shuffle=True, collate_fn=custom_collate)  # Create DataLoader for training\n",
    "# out: 80,000\n",
    "\n",
    "print(\"Validation \", len(val_dataset))\n",
    "val_dataloader = StreamingDataLoader(val_dataset, num_workers=4, batch_size=32, shuffle=False, collate_fn=custom_collate)  # Create DataLoader for validation\n",
    "\n",
    "test_dataloader = StreamingDataLoader(test_dataset, num_workers=4, batch_size=32, shuffle=False, collate_fn=custom_collate)\n",
    "# out: 10,000\n",
    "print(\"Test \", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0a84a",
   "metadata": {},
   "source": [
    "### Define the Model and Training Loop\n",
    "\n",
    "Now that we have the data, we can define the model and training loop. We will use the Mamba SSM model from the mamba package. The model is a simple recurrent neural network (RNN) that is designed to work with sequences of data. It uses a state space model to learn the underlying patterns in the data and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4712444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba2\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim, device=\"cuda\")\n",
    "\n",
    "model = Mamba2(\n",
    "    d_model=dim,\n",
    "    d_state=16,\n",
    "    d_conv=4,\n",
    "    expand=2,\n",
    "    headdim=16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5055fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526630b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
