{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a3bdf6",
   "metadata": {},
   "source": [
    "# Mamba State Space Model with Docker for Local Development and Cloud Deployment\n",
    "\n",
    "This documentation provides a guide on Mamba State Space Model (SSM) implemented in Python, designed for both local development and cloud deployment using Docker. It covers the following topics:\n",
    "\n",
    "1. **Introduction to Mamba SSM**: Overview of the Mamba State Space Model and its applications.\n",
    "2. **Setting Up the Development Environment**: Step-by-step instructions for setting up a local development environment using Docker.\n",
    "3. **Building and Running the Docker Container**: Instructions for building the Docker image and running the container.\n",
    "4. **Deploying to the Cloud**: Guidelines for deploying the Mamba SSM to a cloud platform using Docker.\n",
    "5. **Best Practices**: Tips and best practices for working with Mamba SSM and Docker.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following installed on your local machine:\n",
    "\n",
    "- Docker: [Install Docker](https://docs.docker.com/get-docker/)\n",
    "- A compatible GPU (for Mamba SSM)\n",
    "- NVIDIA drivers (if using GPU)\n",
    "\n",
    "\n",
    "## Sections\n",
    "- [Introduction to Mamba SSM](#introduction-to-mamba-ssm)\n",
    "- [Building and Running the Docker Container](#building-and-running-the-docker-container)\n",
    "- [Using Mamba SSM](#using-mamba-ssm)\n",
    "- [Deploying to the Cloud](#deploying-to-the-cloud)\n",
    "\n",
    "\n",
    "## Introduction to Mamba SSM\n",
    "\n",
    "At the heart of modern AI systems like ChatGPT and AlphaFold lies a critical challenge: understanding sequences. Whether you're parsing a sentence, listening to speech, decoding a genome, or analyzing financial time series, the task boils down to learning patterns across time or space. For the past few years, Transformers—powered by the self-attention mechanism—have dominated this space due to their remarkable ability to model complex relationships between every pair of inputs. But this power comes at a cost: Transformers scale poorly with longer sequences and become expensive or infeasible beyond a few thousand steps.\n",
    "\n",
    "Enter Mamba, a new kind of sequence model inspired by state space systems—an elegant mathematical framework traditionally used in physics and control theory. Think of it like replacing a massive all-to-all attention grid with a sleek, memory-efficient signal processor that knows when to pay attention and when to ignore noise.\n",
    "\n",
    "What makes Mamba different isn’t just that it’s faster (though it is—often 3× faster than its peers on modern GPUs), or that it can handle million-token contexts (which it can, with ease). It’s that Mamba is selective. Unlike older state space models that process information uniformly over time, Mamba can decide what matters at each step. It brings a kind of intelligent filtering—like memory with a spotlight—selectively storing important details and discarding the rest.\n",
    "\n",
    "Here’s the kicker: despite being fully recurrent and operating in linear time, Mamba matches or exceeds the accuracy of Transformers in domains ranging from text and audio to genomics. It doesn’t need attention layers or even separate MLP blocks. Its design is minimal, clean, and hardware-aware—making it not only smart, but fast.\n",
    "\n",
    "By blending the long-term memory of RNNs, the locality of CNNs, and the expressive power of Transformers—all within a scalable, streamlined architecture—Mamba represents a profound shift in how we think about modeling sequences at scale.\n",
    "\n",
    "\n",
    "## Building and Running the Docker Container\n",
    "\n",
    "To build and run the Docker container for Mamba SSM, follow these steps:\n",
    "\n",
    "1. **Clone the Repository**: Clone the Mamba SSM repository to your local machine.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/gabenavarro/MLContainerLab.git\n",
    "   cd MLContainerLab\n",
    "   ```\n",
    "\n",
    "2. **Build the Docker Image**: Use the provided Dockerfile to build the Docker image.\n",
    "\n",
    "   ```bash\n",
    "   # You can choose any tag you want for the image\n",
    "   # Feel free to play around with the base image, just make sure the host has the same or higher CUDA version\n",
    "   docker build -f ./assets/build/Dockerfile.mamba.cu128py26cp312 -t ssm-mamba:128-26-312 .\n",
    "   ```\n",
    "3. **Run the Docker Container**: Run the Docker container with the necessary configurations. In the first example, we will run the container locally with GPU support. This is the recommended way to run a container while in development mode. For scaling up, we will use the second example which runs the container in the cloud.\n",
    "\n",
    "   ```bash\n",
    "    # Run the container with GPU support\n",
    "    docker run -dt \\\n",
    "        --gpus all \\\n",
    "        -v \"$(pwd):/workspace\" \\\n",
    "        --name ssm-mamba \\\n",
    "        --env NVIDIA_VISIBLE_DEVICES=all \\\n",
    "        --env GOOGLE_APPLICATION_CREDENTIALS=/workspace/assets/secrets/gcp-key.json \\\n",
    "        ssm-mamba:128-26-312\n",
    "    ```\n",
    "> Note: The `-v \"$(pwd):/workspace\"` option mounts the current directory to `/workspace` in the container, allowing you to access your local files from within the container. The `--env` options set environment variables for GPU visibility and Google Cloud credentials.<br>\n",
    "> Note: The `--gpus all` option allows the container to use all available GPUs. <br>\n",
    "\n",
    "4. **Access the Container with IDE**: In this example, we will use Visual Studio Code to access the container. You can use any IDE of your choice.\n",
    "\n",
    "   ```bash\n",
    "   # In a scriptable manner\n",
    "   CONTAINER_NAME=ssm-mamba\n",
    "   FOLDER=/workspace\n",
    "   HEX_CONFIG=$(printf {\\\"containerName\\\":\\\"/$CONTAINER_NAME\\\"} | od -A n -t x1 | tr -d '[\\n\\t ]')\n",
    "   code --folder-uri \"vscode-remote://attached-container+$HEX_CONFIG$FOLDER\"\n",
    "   ```\n",
    "\n",
    "> Note: The `code` command is used to open Visual Studio Code. Make sure you have the Remote - Containers extension installed in VS Code to access the container directly. <br>\n",
    "> Note: Make sure you have installed Remote - Containers extension in VS Code.<br>\n",
    "\n",
    "## Using Mamba SSM\n",
    "\n",
    "We will now train a simple Mamba SSM model below using the installed dockerized mamba package above. We will be using a training routine that is similar to training language models with a dataset of bitcoin prices. It will have severe overfitting, but it will be enough to show how to use the package. The training routine is similar to the one used in the [Mamba SSM repository](https://github.com/gabenavarro/MLContainerLab/tree/main/assets/examples/mamba).\n",
    "\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "First, lets go ahead and download the data. We will use a limited dataset from Kaggle to start, however in more advanced scenarios its highly suggested to use an API with access to more datasets such as the [CoinGecko API](https://www.coingecko.com/en/api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8149c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download and unzip the Bitcoin historical data dataset from Kaggle\n",
    "!curl -L -o /workspace/datasets/bitcoin-historical-data.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/mczielinski/bitcoin-historical-data \\\n",
    "    && unzip -o /workspace/datasets/bitcoin-historical-data.zip -d /workspace/datasets/ \\\n",
    "    && rm /workspace/datasets/bitcoin-historical-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f0cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SERIES_CSV = \"/workspace/datasets/btcusd_1-min_data.csv\"\n",
    "PROCESSED_DATA_DIR = \"/workspace/datasets/auto_regressive_processed_timeseries\"\n",
    "CKPT_DIR = \"/workspace/datasets/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5bb2fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:01:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:02:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:04:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:05:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Timestamp  Open  High   Low  Close  Volume                   datetime\n",
       "0  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:01:00+00:00\n",
       "1  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:02:00+00:00\n",
       "2  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:03:00+00:00\n",
       "3  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:04:00+00:00\n",
       "4  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:05:00+00:00"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.read_csv(TIME_SERIES_CSV, low_memory=False, nrows=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c5ded",
   "metadata": {},
   "source": [
    "Now that we have the dataset, lets create a dataset and dataloader for the model using litdata. Litdata is a lightweight data loading library that is designed to work with PyTorch and other deep learning frameworks. It provides a simple and efficient way to load and preprocess data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14778d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import litdata as ld\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "def process_timeseries(file_path: str, sequence_length: int = 2048) -> Dict[str, Any]:\n",
    "    \"\"\"Process a timeseries CSV file into a format suitable for autoregressive modeling.\"\"\"\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    if \"datetime\" in df.columns:\n",
    "        df = df.drop(columns=[\"datetime\"])\n",
    "    df = df.sort_values('Timestamp')\n",
    "    numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # —— NEW: log-transform price values to handle exponential growth ——\n",
    "    for price_col in ['Open', 'High', 'Low', 'Close']:\n",
    "        # Ensure all values are positive before log transform\n",
    "        df[price_col] = np.log(df[price_col].replace([np.inf, -np.inf, 0, np.nan], 0.01).fillna(0.01))\n",
    "    \n",
    "    # —— NEW: log1p-transform volume to reduce extreme skew ——\n",
    "    df['Volume'] = np.log1p(df['Volume'].replace([np.inf, -np.inf], np.nan).fillna(0.0))\n",
    "    \n",
    "    stats = {}\n",
    "    for feature in numerical_features:\n",
    "        vals = df[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        mean, std = (vals.mean(), vals.std() or 1.0)\n",
    "        stats[feature] = {'mean': mean, 'std': std}\n",
    "        df[feature] = (df[feature] - mean) / std\n",
    "\n",
    "    if len(df) <= sequence_length:\n",
    "        print(f\"Warning: only {len(df)} rows < sequence_length={sequence_length}\")\n",
    "        return None\n",
    "\n",
    "    def create_timeseries_sample(index: int) -> Dict[str, Any]:\n",
    "        if index < sequence_length or index >= len(df):\n",
    "            return {\"index\": index,\n",
    "                    \"inputs\": np.zeros((sequence_length,5),dtype=np.float32),\n",
    "                    \"mask\": np.zeros(sequence_length, dtype=bool),\n",
    "                    \"stats\": stats}\n",
    "        seq = df.iloc[index-sequence_length:index][numerical_features].values\n",
    "        arr = np.nan_to_num(seq.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mask = ~np.isnan(seq).any(axis=1)\n",
    "        return {\"index\": index, \"inputs\": arr, \"mask\": mask, \"stats\": stats}\n",
    "\n",
    "    # Store original stats for inverse transformation during inference\n",
    "    stats['transform_type'] = 'log_then_zscore'\n",
    "    \n",
    "    return create_timeseries_sample\n",
    "\n",
    "\n",
    "# Set the sequence length for your model\n",
    "sequence_length = 2048\n",
    "\n",
    "# Get the processing function configured for your specific file\n",
    "process_function = process_timeseries(TIME_SERIES_CSV, sequence_length)\n",
    "\n",
    "# Filter indices to exclude those with NaN values\n",
    "def get_valid_indices():\n",
    "    df = pd.read_csv(TIME_SERIES_CSV, low_memory=False)\n",
    "    if \"datetime\" in df.columns:\n",
    "        df = df.drop(columns=[\"datetime\"])\n",
    "    \n",
    "    df = df.sort_values('Timestamp')\n",
    "    numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Replace infinity values with NaN\n",
    "    for feature in numerical_features:\n",
    "        df[feature] = df[feature].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    valid_indices = []\n",
    "    for idx in range(sequence_length, len(df), int(sequence_length * 0.25)):\n",
    "        # Check if the entire sequence has no NaN values\n",
    "        sequence = df.iloc[idx-sequence_length:idx][numerical_features].values\n",
    "        if not np.isnan(sequence).any() and not np.isinf(sequence).any():\n",
    "            valid_indices.append(idx)\n",
    "        else:\n",
    "            print(f\"Skipping index {idx} due to NaN or inf values in the sequence.\")\n",
    "    \n",
    "    if not valid_indices:\n",
    "        raise ValueError(\"No valid sequences found! All sequences contain NaN or inf values.\")\n",
    "    \n",
    "    return valid_indices\n",
    "\n",
    "valid_indices = get_valid_indices()\n",
    "\n",
    "# The optimize function writes data in an optimized format\n",
    "ld.optimize(\n",
    "    fn=process_function,              # the function that processes each sample\n",
    "    inputs=valid_indices,             # the indices of valid samples\n",
    "    output_dir=PROCESSED_DATA_DIR,    # optimized data is stored here\n",
    "    num_workers=4,                    # The number of workers on the same machine\n",
    "    chunk_bytes=\"64MB\"                # size of each chunk\n",
    ")\n",
    "# Takes about 30 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2a95fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3577: UserWarning: A newer version of litdata is available (0.2.47). Please consider upgrading with `pip install -U litdata`. Not all functionalities of the platform can be guaranteed to work with the current version.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13690\n",
      "Train  10952\n",
      "Validation  1369\n",
      "Test  1369\n"
     ]
    }
   ],
   "source": [
    "from litdata import StreamingDataset, StreamingDataLoader, train_test_split\n",
    "import torch\n",
    "streaming_dataset = StreamingDataset(PROCESSED_DATA_DIR) # data are stored in the cloud\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        # Return empty tensors if the batch is empty\n",
    "        return {\n",
    "            \"index\": torch.tensor([], dtype=torch.long),\n",
    "            \"inputs\": torch.tensor([], dtype=torch.float32),\n",
    "            \"mask\": torch.tensor([], dtype=torch.bool),\n",
    "            \"stats\": {}\n",
    "        }\n",
    "    \n",
    "    # Process each key separately\n",
    "    indices = torch.tensor([item[\"index\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # Make sure arrays are writable by copying and convert to tensor\n",
    "    inputs = torch.stack([torch.tensor(np.nan_to_num(item[\"inputs\"].copy(), nan=0.0), dtype=torch.float32) for item in batch])\n",
    "    masks = torch.stack([torch.tensor(item[\"mask\"].copy(), dtype=torch.bool) for item in batch])\n",
    "    \n",
    "    # Get stats (use first non-empty item's stats)\n",
    "    stats = next((item[\"stats\"] for item in batch if \"stats\" in item), {})\n",
    "    \n",
    "    return {\n",
    "        \"index\": indices,\n",
    "        \"inputs\": inputs,\n",
    "        \"mask\": masks,\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "print(len(streaming_dataset)) # display the length of your data\n",
    "# out: 100,000\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = train_test_split(streaming_dataset, splits=[0.8, 0.1, 0.1])\n",
    "\n",
    "print(\"Train \", len(train_dataset))\n",
    "train_dataloader = StreamingDataLoader(train_dataset, num_workers=4, batch_size=32, shuffle=True, collate_fn=custom_collate)  # Create DataLoader for training\n",
    "# out: 80,000\n",
    "\n",
    "print(\"Validation \", len(val_dataset))\n",
    "val_dataloader = StreamingDataLoader(val_dataset, num_workers=4, batch_size=32, shuffle=False, collate_fn=custom_collate)  # Create DataLoader for validation\n",
    "\n",
    "test_dataloader = StreamingDataLoader(test_dataset, num_workers=4, batch_size=32, shuffle=False, collate_fn=custom_collate)\n",
    "# out: 10,000\n",
    "print(\"Test \", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39067c",
   "metadata": {},
   "source": [
    "### Define the Model and Training Loop\n",
    "\n",
    "Now that we have the data, we can define the model and training loop. We will use the Mamba SSM model from the mamba package. The model is a simple recurrent neural network (RNN) that is designed to work with sequences of data. It uses a state space model to learn the underlying patterns in the data and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8064f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm import Mamba2\n",
    "from torch import nn, optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "import lightning as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "\n",
    "class Mamba2Layer(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_model:int, d_state:int, d_conv:int, expand:int, headdim:int,\n",
    "            layer_idx:int\n",
    "        ):\n",
    "        super(Mamba2Layer, self).__init__()\n",
    "        self.mamba = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            headdim=headdim,\n",
    "            layer_idx=layer_idx,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.mamba(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ebae8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba2Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self, input_dim:int, d_model:int, d_state:int, d_conv:int, \n",
    "            expand:int, headdim:int, n_layers:int, \n",
    "            lr:float=1e-4, weight_decay:float=0.01, **kwargs\n",
    "        ):\n",
    "        super(Mamba2Model, self).__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Linear layer for input projection\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Mamba2 layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            Mamba2Layer(d_model, d_state, d_conv, expand, headdim, i) for i in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Linear layer for output projection\n",
    "        self.linear = nn.Linear(d_model, input_dim)\n",
    "\n",
    "        # Provide a small weight for the volume feature\n",
    "        fw = torch.tensor([1.,1.,1.,1.,0.25])\n",
    "        self.register_buffer('feature_weights', fw)\n",
    "\n",
    "        # For tracking metrics\n",
    "        self.train_loss = 0.0\n",
    "        self.val_loss = 0.0\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # For storing test predictions\n",
    "        self.test_predictions = []\n",
    "        self.test_targets = []\n",
    "        self.test_masks = []\n",
    "        self.feature_names = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.input_projection(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def _calculate_autoregressive_loss(self, preds, targets, mask):\n",
    "        # shift for next-step prediction\n",
    "        p, t = preds[:, :-1], targets[:, 1:]\n",
    "        m = mask[:, 1:].unsqueeze(-1)\n",
    "        # mse = F.mse_loss(p, t, reduction='none')\n",
    "        mse = F.smooth_l1_loss(p, t, reduction='none', beta=0.01)\n",
    "        # apply per-feature weights\n",
    "        mse = mse * self.feature_weights.view(1,1,-1)\n",
    "        # mask and reduce\n",
    "        mse = mse * m\n",
    "        denom = m.sum().clamp_min(1.0)\n",
    "        return mse.sum() / denom\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step for autoregressive prediction\n",
    "        \"\"\"\n",
    "        # Get inputs and mask from batch\n",
    "        inputs = batch['inputs']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        # Safety check for NaN in inputs\n",
    "        if torch.isnan(inputs).any():\n",
    "            inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "        \n",
    "        # Forward pass to get predictions\n",
    "        predictions = self(inputs)\n",
    "        \n",
    "        # Calculate autoregressive loss\n",
    "        loss = self._calculate_autoregressive_loss(predictions, inputs, mask)        \n",
    "        \n",
    "        # Log the loss for monitoring (don't try to compute gradient norm here)\n",
    "        self.train_loss = loss\n",
    "        self.log('train_loss', loss, prog_bar=True)  \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_after_backward(self):\n",
    "        \"\"\"\n",
    "        Called after .backward() and before optimizers do anything.\n",
    "        This is the right place to check gradients.\n",
    "        \"\"\"\n",
    "        # Safely compute gradient norm - after backward pass when gradients exist\n",
    "        if any(p.grad is not None for p in self.parameters()):\n",
    "            grad_list = [p.grad.detach().norm(2) for p in self.parameters() if p.grad is not None]\n",
    "            if grad_list:  # Make sure list is not empty\n",
    "                grad_norm = torch.stack(grad_list).norm(2)\n",
    "                self.log('grad_norm', grad_norm, prog_bar=True)\n",
    "            else:\n",
    "                self.log('grad_norm', torch.tensor(0.0), prog_bar=True)\n",
    "        else:\n",
    "            self.log('grad_norm', torch.tensor(0.0), prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step for autoregressive prediction\n",
    "        \"\"\"\n",
    "        # Get inputs and mask from batch\n",
    "        inputs = batch['inputs']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        # Safety check for NaN in inputs\n",
    "        if torch.isnan(inputs).any():\n",
    "            inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "        \n",
    "        # Forward pass to get predictions\n",
    "        predictions = self(inputs)\n",
    "        \n",
    "        # Calculate autoregressive loss\n",
    "        loss = self._calculate_autoregressive_loss(predictions, inputs, mask)\n",
    "                \n",
    "        # Log the loss for monitoring\n",
    "        self.val_loss = loss\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step for evaluating the model after training\n",
    "        \"\"\"\n",
    "        # Get inputs and mask from batch\n",
    "        inputs = batch['inputs']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        # Safety check for NaN in inputs\n",
    "        if torch.isnan(inputs).any():\n",
    "            inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "        \n",
    "        # Forward pass to get predictions\n",
    "        predictions = self(inputs)\n",
    "        \n",
    "        # Calculate autoregressive loss\n",
    "        loss = self._calculate_autoregressive_loss(predictions, inputs, mask)\n",
    "        \n",
    "        # Store predictions and targets for later analysis\n",
    "        # Use detach() and cpu() to avoid memory leaks\n",
    "        self.test_predictions.append(predictions.detach().cpu())\n",
    "        self.test_targets.append(inputs.detach().cpu())\n",
    "        self.test_masks.append(mask.detach().cpu())\n",
    "        \n",
    "        # Log the test loss\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # Configuring the optimizer\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": opt,\n",
    "            \"lr_scheduler_config\": {\n",
    "                \"scheduler\": sch,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Calculate and log test metrics at the end of the test epoch\n",
    "        \"\"\"\n",
    "        # Concatenate all batches\n",
    "        all_preds = torch.cat(self.test_predictions, dim=0)\n",
    "        all_targets = torch.cat(self.test_targets, dim=0)\n",
    "        all_masks = torch.cat(self.test_masks, dim=0)\n",
    "        \n",
    "        # For autoregressive prediction, shift by one step\n",
    "        pred_shifted = all_preds[:, :-1, :]\n",
    "        target_shifted = all_targets[:, 1:, :]\n",
    "        mask_shifted = all_masks[:, 1:]\n",
    "        \n",
    "        # Convert to numpy for sklearn metrics\n",
    "        pred_np   = pred_shifted.detach().cpu().to(torch.float32).numpy()\n",
    "        target_np = target_shifted.detach().cpu().to(torch.float32).numpy()\n",
    "        mask_np   = mask_shifted.detach().cpu().to(torch.float32).numpy()\n",
    "        \n",
    "        # Calculate metrics for each feature\n",
    "        metrics = {}\n",
    "        for i, feature_name in enumerate(self.feature_names):\n",
    "            # Extract predictions and targets for this feature\n",
    "            feature_preds = pred_np[:, :, i]\n",
    "            feature_targets = target_np[:, :, i]\n",
    "            \n",
    "            # Apply mask to consider only valid positions\n",
    "            valid_preds = []\n",
    "            valid_targets = []\n",
    "            \n",
    "            # Flatten and filter by mask\n",
    "            for batch_idx in range(feature_preds.shape[0]):\n",
    "                for seq_idx in range(feature_preds.shape[1]):\n",
    "                    if mask_np[batch_idx, seq_idx]:\n",
    "                        valid_preds.append(feature_preds[batch_idx, seq_idx])\n",
    "                        valid_targets.append(feature_targets[batch_idx, seq_idx])\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            valid_preds = np.array(valid_preds)\n",
    "            valid_targets = np.array(valid_targets)\n",
    "            \n",
    "            if len(valid_preds) > 0:\n",
    "                # Calculate metrics\n",
    "                mse = mean_squared_error(valid_targets, valid_preds)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(valid_targets, valid_preds)\n",
    "                \n",
    "                # Calculate MAPE (Mean Absolute Percentage Error) with handling for zeros\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    mape = np.mean(np.abs((valid_targets - valid_preds) / np.maximum(np.abs(valid_targets), 1e-8))) * 100\n",
    "                    mape = np.nan_to_num(mape, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                \n",
    "                # R-squared\n",
    "                r2 = r2_score(valid_targets, valid_preds)\n",
    "                \n",
    "                # Store metrics\n",
    "                metrics[f\"{feature_name}_mse\"] = mse\n",
    "                metrics[f\"{feature_name}_rmse\"] = rmse\n",
    "                metrics[f\"{feature_name}_mae\"] = mae\n",
    "                metrics[f\"{feature_name}_mape\"] = mape\n",
    "                metrics[f\"{feature_name}_r2\"] = r2\n",
    "                \n",
    "                # Log each metric\n",
    "                self.log(f\"test_{feature_name}_mse\", mse)\n",
    "                self.log(f\"test_{feature_name}_rmse\", rmse)\n",
    "                self.log(f\"test_{feature_name}_mae\", mae)\n",
    "                self.log(f\"test_{feature_name}_mape\", mape)\n",
    "                self.log(f\"test_{feature_name}_r2\", r2)\n",
    "        \n",
    "        # Calculate average metrics across all features\n",
    "        avg_mse = np.mean([metrics[f\"{feature}_mse\"] for feature in self.feature_names if f\"{feature}_mse\" in metrics])\n",
    "        avg_rmse = np.mean([metrics[f\"{feature}_rmse\"] for feature in self.feature_names if f\"{feature}_rmse\" in metrics])\n",
    "        avg_mae = np.mean([metrics[f\"{feature}_mae\"] for feature in self.feature_names if f\"{feature}_mae\" in metrics])\n",
    "        avg_mape = np.mean([metrics[f\"{feature}_mape\"] for feature in self.feature_names if f\"{feature}_mape\" in metrics])\n",
    "        avg_r2 = np.mean([metrics[f\"{feature}_r2\"] for feature in self.feature_names if f\"{feature}_r2\" in metrics])\n",
    "        \n",
    "        # Log average metrics\n",
    "        self.log(\"test_avg_mse\", avg_mse)\n",
    "        self.log(\"test_avg_rmse\", avg_rmse)\n",
    "        self.log(\"test_avg_mae\", avg_mae)\n",
    "        self.log(\"test_avg_mape\", avg_mape)\n",
    "        self.log(\"test_avg_r2\", avg_r2)\n",
    "        \n",
    "        # Print summary of test metrics\n",
    "        print(\"\\n===== TEST METRICS =====\")\n",
    "        print(f\"Average MSE: {avg_mse:.6f}\")\n",
    "        print(f\"Average RMSE: {avg_rmse:.6f}\")\n",
    "        print(f\"Average MAE: {avg_mae:.6f}\")\n",
    "        print(f\"Average MAPE: {avg_mape:.6f}%\")\n",
    "        print(f\"Average R²: {avg_r2:.6f}\")\n",
    "        print(\"=======================\\n\")\n",
    "        \n",
    "        # Create and save visualizations\n",
    "        self._create_prediction_visualizations(pred_shifted, target_shifted, mask_shifted)\n",
    "        \n",
    "        # Clear stored predictions to free memory\n",
    "        self.test_predictions = []\n",
    "        self.test_targets = []\n",
    "        self.test_masks = []\n",
    "\n",
    "    def _create_prediction_visualizations(self, predictions, targets, masks, num_samples=3):\n",
    "        \"\"\"\n",
    "        Create and save visualization of predictions vs targets\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions [batch_size, seq_len, feature_dim]\n",
    "            targets: Target values [batch_size, seq_len, feature_dim]\n",
    "            masks: Boolean masks [batch_size, seq_len]\n",
    "            num_samples: Number of samples to visualize\n",
    "        \"\"\"\n",
    "        # Convert to numpy for plotting\n",
    "        preds_np = predictions.detach().cpu().to(torch.float32).numpy()\n",
    "        targets_np = targets.detach().cpu().to(torch.float32).numpy()\n",
    "        masks_np = masks.detach().cpu().to(torch.float32).numpy()\n",
    "        \n",
    "        # Create directory for plots if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs(os.path.join(CKPT_DIR, \"plots\"), exist_ok=True)\n",
    "        \n",
    "        # Plot for each feature\n",
    "        for feature_idx, feature_name in enumerate(self.feature_names):\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            \n",
    "            # Plot for a few random samples\n",
    "            for sample_idx in range(min(num_samples, preds_np.shape[0])):\n",
    "                # Get predictions and targets for this sample and feature\n",
    "                sample_preds = preds_np[sample_idx, :, feature_idx]\n",
    "                sample_targets = targets_np[sample_idx, :, feature_idx]\n",
    "                sample_mask = masks_np[sample_idx, :]\n",
    "                \n",
    "                # Create time index for x-axis\n",
    "                time_idx = np.arange(len(sample_preds))\n",
    "                \n",
    "                # Plot targets\n",
    "                plt.subplot(num_samples, 1, sample_idx + 1)\n",
    "                plt.plot(time_idx, sample_targets, 'b-', label='Actual', alpha=0.7)\n",
    "                \n",
    "                # Plot predictions (only where mask is True)\n",
    "                masked_preds = np.where(sample_mask, sample_preds, np.nan)\n",
    "                plt.plot(time_idx, masked_preds, 'r-', label='Predicted', alpha=0.7)\n",
    "                \n",
    "                # Add title and legend\n",
    "                plt.title(f\"Sample {sample_idx+1}: {feature_name}\")\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Calculate metrics for this sample\n",
    "                valid_indices = np.where(sample_mask)[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    valid_preds = sample_preds[valid_indices]\n",
    "                    valid_targets = sample_targets[valid_indices]\n",
    "                    \n",
    "                    mse = mean_squared_error(valid_targets, valid_preds)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    r2 = r2_score(valid_targets, valid_preds)\n",
    "                    \n",
    "                    plt.figtext(0.01, 0.5 - 0.15 * sample_idx, \n",
    "                                f\"MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\", \n",
    "                                fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(CKPT_DIR, \"plots\", f\"{feature_name}_predictions.svg\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Create a summary plot with all features for the first sample\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        for feature_idx, feature_name in enumerate(self.feature_names):\n",
    "            plt.subplot(len(self.feature_names), 1, feature_idx + 1)\n",
    "            \n",
    "            # Get predictions and targets for first sample and this feature\n",
    "            sample_preds = preds_np[0, :, feature_idx]\n",
    "            sample_targets = targets_np[0, :, feature_idx]\n",
    "            sample_mask = masks_np[0, :]\n",
    "            \n",
    "            # Create time index for x-axis\n",
    "            time_idx = np.arange(len(sample_preds))\n",
    "            \n",
    "            # Plot targets\n",
    "            plt.plot(time_idx, sample_targets, 'b-', label='Actual', alpha=0.7)\n",
    "            \n",
    "            # Plot predictions (only where mask is True)\n",
    "            masked_preds = np.where(sample_mask, sample_preds, np.nan)\n",
    "            plt.plot(time_idx, masked_preds, 'r-', label='Predicted', alpha=0.7)\n",
    "            \n",
    "            # Add title and legend\n",
    "            plt.title(f\"{feature_name}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CKPT_DIR, \"plots\", \"all_features_predictions.svg\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "849fb9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | input_projection | Linear     | 384    | train\n",
      "1 | layers           | ModuleList | 242 K  | train\n",
      "2 | linear           | Linear     | 325    | train\n",
      "--------------------------------------------------------\n",
      "243 K     Trainable params\n",
      "0         Non-trainable params\n",
      "243 K     Total params\n",
      "0.972     Total estimated model params size (MB)\n",
      "31        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cb3be43228499fb98f4cd0a2393b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93dc8be9d754284964063fcc1c6d64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8d1e31952546dba775cf3add38d045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 25. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36ecae5327e4b0eaf0f607b8faa2dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06cd24c0f2342e6af318e5f10e3e1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdffe72dec41463ab7f4be8b8478d762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5032be2fd8e74a078fe42c993d0efa86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab82e401f104b2aa0c0e1c44e71d852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97911b08a95b4a418eb81f84ad0c9974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a312f165bf7a4cb9ad521aa936037b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbf1f4854e74a7a8d33f0d34f09c215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c87dbcf9d894354bba0aafa7d4cafd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759aa342e2c64a69a8149bcd29b8c42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [-1:59:59<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24c59eb38224c7999a27581e62b2701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53295bc1c3684c21acbc4cd2da57ad5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3340f084544ac0bdbc1aa8df57fa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88c8956762a4697b72db2f1c1dfa315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e6b0dbda3f45fb96e80d5b42acb38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8ca69036594d498a54660efb1fbab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b4193f5a6744428b24b9a08df0be63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0bad8ccc9149188a5fbe3ffb248532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e05a028a22040bfa1236dbcf09f9c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c689c1e7622f489dbbeac0bf5687ef6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3461a29910dd478589eeef97ff01ea2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0314973c7014f70816da1ec637b8c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849f42ea661c422db1ab9de6302e1d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfd1190a2ae4f42875be22f5b9f9c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00eaf6e818cd41bc8c0b52935461ffcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063e95e0240642aa90b1541e4a91ad8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb6d9091cb34951abba15d67b234a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecad19b9484444dbe5bff0f8fcc90de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adad4767d8574501bc340a5d4d11abd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c485ab4afd4c70bb62603f71b3b178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61f6f2cb3ce47fc97f5c6bdc1baa448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = dict(\n",
    "    input_dim=5,\n",
    "    d_model=64, \n",
    "    d_state=64, \n",
    "    d_conv=4, \n",
    "    expand=4, \n",
    "    headdim=16, \n",
    "    n_layers=4,\n",
    "    lr=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    epochs=100,\n",
    "    data_dir = PROCESSED_DATA_DIR,  # Directory for processed data\n",
    "    ckpt_dir = CKPT_DIR,            # Checkpoint directory\n",
    "    num_workers = 4,                # Number of workers for data loading\n",
    "    batch_size = 32,                # Batch size\n",
    "    sequence_length = 2048,         # Sequence length\n",
    "    limit_val_batches = 100,        # Use a fraction of validation data for faster training\n",
    "    accumulate_grad_batches = 8,    # Gradient accumulation\n",
    "    gradient_clip_val = 0.5,        # Gradient clipping for stability\n",
    "    precision = \"bf16-mixed\",       # Mixed precision training\n",
    "    log_every_n_steps = 10,         # Log every n steps\n",
    "    accelerator = \"gpu\",            # Use GPU for training\n",
    "    nodes = 1,                      # Number of nodes\n",
    "    devices = 1,                    # Number of devices (GPUs)\n",
    "    strategy = \"auto\",              # Distributed Data Parallel\n",
    "\n",
    ")\n",
    "model = Mamba2Model(**parameters)\n",
    "\n",
    "\n",
    "\n",
    "# Lightning callbacks for early stopping and model checkpointing\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=parameters.get('ckpt_dir', CKPT_DIR),\n",
    "    filename='mamba-best-model-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    limit_val_batches=parameters['limit_val_batches'],\n",
    "    max_epochs=parameters['epochs'], \n",
    "    accumulate_grad_batches=parameters['accumulate_grad_batches'], \n",
    "    gradient_clip_val=parameters['gradient_clip_val'],  \n",
    "    default_root_dir=parameters['ckpt_dir'],\n",
    "    precision=parameters['precision'],\n",
    "    log_every_n_steps=parameters['log_every_n_steps'],\n",
    "    accelerator=parameters['accelerator'],\n",
    "    devices=parameters['devices'],\n",
    "    strategy=parameters['strategy'],\n",
    "    num_nodes=parameters['nodes'],\n",
    "    callbacks=[early_stop_callback, checkpoint_callback]\n",
    ") \n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1f9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_tensorboard_scalars(ckpt_dir:str, tag=None, tags=None):\n",
    "    \"\"\"\n",
    "    Plot scalar curves from TensorBoard logs in a given directory.\n",
    "\n",
    "    Args:\n",
    "        tag (str): Specific tag to plot.\n",
    "        tags (list): List of tags to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "    \n",
    "    # Find the most recent version\n",
    "    version_dirs = glob.glob(f\"{ckpt_dir}/lightning_logs/version_*\")\n",
    "    if not version_dirs:\n",
    "        print(\"No training logs found.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    latest_version = max(version_dirs, key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "    # Find any event files in the directory\n",
    "    files = glob.glob(os.path.join(latest_version, 'events.out.tfevents.*'))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No TensorBoard event files found in {latest_version}\")\n",
    "\n",
    "    # Initialize the EventAccumulator to read scalars\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        latest_version,\n",
    "        size_guidance={  # Load all scalar data\n",
    "            event_accumulator.SCALARS: 0,\n",
    "        }\n",
    "    )\n",
    "    ea.Reload()  # Load the event data\n",
    "\n",
    "    # Determine which tags to plot\n",
    "    available_tags = ea.Tags().get('scalars', [])\n",
    "    if tags:\n",
    "        plot_tags = [t for t in tags if t in available_tags]\n",
    "    elif tag:\n",
    "        if tag not in available_tags:\n",
    "            raise ValueError(f\"Tag '{tag}' not found. Available tags: {available_tags}\")\n",
    "        plot_tags = [tag]\n",
    "    else:\n",
    "        # If no tag(s) specified, plot all scalar tags\n",
    "        plot_tags = available_tags\n",
    "\n",
    "    if not plot_tags:\n",
    "        raise ValueError(\"No valid scalar tags to plot.\")\n",
    "\n",
    "    # Plot each tag's values over steps\n",
    "    plt.figure()\n",
    "    for t in plot_tags:\n",
    "        events = ea.Scalars(t)\n",
    "        steps = [e.step for e in events]\n",
    "        values = [e.value for e in events]\n",
    "        plt.plot(steps, values, label=t)\n",
    "\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f\"TensorBoard Scalars\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ckpt_dir, \"plots\", \"mamba_tensorBoard_scalars.svg\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plot_tensorboard_scalars(ckpt_dir=parameters['ckpt_dir'], tags=['train_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4170d",
   "metadata": {},
   "source": [
    "Below is the scalar plot of the training and validation loss over the epochs. The training loss is shown in blue and the validation loss is shown in orange. As we can see, the training loss decreases over time, indicating that the model is learning. The validation loss also decreases, but at a slower rate, indicating that the model is not overfitting to the training data.\n",
    "\n",
    "![Mamba Attention Predictions](../assets/images/mamba_tensorBoard_scalars.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f378db",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "\n",
    "### Test Dataset\n",
    "Now that we have trained the model, we can evaluate it on the test dataset. We will use the `evaluate` method from the `litdata` library to evaluate the model on the test dataset. Below is the code to evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ca92beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from /workspace/datasets/checkpoints/mamba-best-model-epoch=21-val_loss=0.1543.ckpt\n",
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398eac1cbabd45ba9c91a67d64bd824b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEST METRICS =====\n",
      "Average MSE: 0.252570\n",
      "Average RMSE: 0.234710\n",
      "Average MAE: 0.161060\n",
      "Average MAPE: 67.145914%\n",
      "Average R²: 0.832838\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Close_mae       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.009779187850654125    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Close_mape      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4570692777633667     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Close_mse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0001886643876787275   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Close_r2       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9970526695251465     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Close_rmse      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.013735515996813774    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_High_mae       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0075338599272072315   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_High_mape       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.0993212461471558     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_High_mse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.000122522353194654    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_High_r2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9980871081352234     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_High_rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.011068981140851974    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Low_mae        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.008318182080984116    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Low_mape       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2755264043807983     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Low_mse        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  0.00015940971206873655   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_Low_r2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9975081086158752     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Low_rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.012625755742192268    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Open_mae       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.009417413733899593    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Open_mape       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.3162744045257568     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Open_mse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">  0.00015961345343384892   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Open_r2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9975064992904663     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Open_rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.012633821927011013    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Volume_mae      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7702534794807434     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_Volume_mape      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     330.5813903808594     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Volume_mse      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2622184753417969     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Volume_r2       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.17403554916381836    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_Volume_rmse      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1234849691390991     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_avg_mae        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1610604226589203     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_avg_mape       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     67.14591217041016     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_avg_mse        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2525697350502014     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_avg_r2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8328379988670349     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_avg_rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.23470981419086456    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.21124550700187683    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Close_mae      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.009779187850654125   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Close_mape     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4570692777633667    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Close_mse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0001886643876787275  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Close_r2      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9970526695251465    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Close_rmse     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.013735515996813774   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_High_mae      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0075338599272072315  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_High_mape      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.0993212461471558    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_High_mse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.000122522353194654   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_High_r2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9980871081352234    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_High_rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.011068981140851974   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Low_mae       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.008318182080984116   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Low_mape      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2755264043807983    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Low_mse       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 0.00015940971206873655  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_Low_r2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9975081086158752    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Low_rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.012625755742192268   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Open_mae      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.009417413733899593   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Open_mape      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.3162744045257568    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Open_mse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m 0.00015961345343384892  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Open_r2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9975064992904663    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Open_rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.012633821927011013   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Volume_mae     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7702534794807434    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_Volume_mape     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    330.5813903808594    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Volume_mse     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2622184753417969    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Volume_r2      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.17403554916381836   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_Volume_rmse     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1234849691390991    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_avg_mae       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1610604226589203    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_avg_mape      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    67.14591217041016    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_avg_mse       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2525697350502014    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_avg_r2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8328379988670349    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_avg_rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.23470981419086456   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.21124550700187683   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.21124550700187683,\n",
       "  'test_Open_mse': 0.00015961345343384892,\n",
       "  'test_Open_rmse': 0.012633821927011013,\n",
       "  'test_Open_mae': 0.009417413733899593,\n",
       "  'test_Open_mape': 1.3162744045257568,\n",
       "  'test_Open_r2': 0.9975064992904663,\n",
       "  'test_High_mse': 0.000122522353194654,\n",
       "  'test_High_rmse': 0.011068981140851974,\n",
       "  'test_High_mae': 0.0075338599272072315,\n",
       "  'test_High_mape': 1.0993212461471558,\n",
       "  'test_High_r2': 0.9980871081352234,\n",
       "  'test_Low_mse': 0.00015940971206873655,\n",
       "  'test_Low_rmse': 0.012625755742192268,\n",
       "  'test_Low_mae': 0.008318182080984116,\n",
       "  'test_Low_mape': 1.2755264043807983,\n",
       "  'test_Low_r2': 0.9975081086158752,\n",
       "  'test_Close_mse': 0.0001886643876787275,\n",
       "  'test_Close_rmse': 0.013735515996813774,\n",
       "  'test_Close_mae': 0.009779187850654125,\n",
       "  'test_Close_mape': 1.4570692777633667,\n",
       "  'test_Close_r2': 0.9970526695251465,\n",
       "  'test_Volume_mse': 1.2622184753417969,\n",
       "  'test_Volume_rmse': 1.1234849691390991,\n",
       "  'test_Volume_mae': 0.7702534794807434,\n",
       "  'test_Volume_mape': 330.5813903808594,\n",
       "  'test_Volume_r2': 0.17403554916381836,\n",
       "  'test_avg_mse': 0.2525697350502014,\n",
       "  'test_avg_rmse': 0.23470981419086456,\n",
       "  'test_avg_mae': 0.1610604226589203,\n",
       "  'test_avg_mape': 67.14591217041016,\n",
       "  'test_avg_r2': 0.8328379988670349}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "if best_model_path:\n",
    "    print(f\"Loading best model from {best_model_path}\")\n",
    "    model = Mamba2Model.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Test the model and calculate metrics\n",
    "print(\"Evaluating model on test set...\")\n",
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "790a9321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metrics file found at /workspace/datasets/checkpoints/lightning_logs/version_3/metrics.csv\n",
      "Test performance report saved to /workspace/datasets/checkpoints/test_performance_report.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# Create a performance report with all the metrics\n",
    "def create_performance_report(ckpt_dir: str = CKPT_DIR, model_name: str = \"mamba\"):\n",
    "    # Get metrics from the lightning logs    \n",
    "    # Find the most recent version\n",
    "    version_dirs = glob.glob(f\"{ckpt_dir}/lightning_logs/version_*\")\n",
    "    if not version_dirs:\n",
    "        print(\"No training logs found.\")\n",
    "        return\n",
    "    \n",
    "    latest_version = max(version_dirs, key=lambda x: int(x.split('_')[-1]))\n",
    "    metrics_path = f\"{latest_version}/metrics.csv\"\n",
    "    \n",
    "    if not os.path.exists(metrics_path):\n",
    "        print(f\"No metrics file found at {metrics_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load metrics\n",
    "    metrics_df = pd.read_csv(metrics_path)\n",
    "    \n",
    "    # Filter for test metrics only\n",
    "    test_metrics = metrics_df[metrics_df['step'].isna()]\n",
    "    \n",
    "    # Create a report\n",
    "    with open(f\"{ckpt_dir}/test_performance_report.md\", 'w') as f:\n",
    "        f.write(\"# Model Performance Report\\n\\n\")\n",
    "        f.write(\"## Overall Metrics\\n\\n\")\n",
    "        \n",
    "        # Add overall metrics\n",
    "        if 'test_avg_mse' in test_metrics.columns:\n",
    "            f.write(f\"* **Average MSE:** {test_metrics['test_avg_mse'].iloc[-1]:.6f}\\n\")\n",
    "        if 'test_avg_rmse' in test_metrics.columns:\n",
    "            f.write(f\"* **Average RMSE:** {test_metrics['test_avg_rmse'].iloc[-1]:.6f}\\n\")\n",
    "        if 'test_avg_mae' in test_metrics.columns:\n",
    "            f.write(f\"* **Average MAE:** {test_metrics['test_avg_mae'].iloc[-1]:.6f}\\n\")\n",
    "        if 'test_avg_mape' in test_metrics.columns:\n",
    "            f.write(f\"* **Average MAPE:** {test_metrics['test_avg_mape'].iloc[-1]:.6f}%\\n\")\n",
    "        if 'test_avg_r2' in test_metrics.columns:\n",
    "            f.write(f\"* **Average R²:** {test_metrics['test_avg_r2'].iloc[-1]:.6f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Feature-Specific Metrics\\n\\n\")\n",
    "        \n",
    "        # Add feature-specific metrics\n",
    "        for feature in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "            f.write(f\"### {feature}\\n\\n\")\n",
    "            \n",
    "            if f'test_{feature}_mse' in test_metrics.columns:\n",
    "                f.write(f\"* **MSE:** {test_metrics[f'test_{feature}_mse'].iloc[-1]:.6f}\\n\")\n",
    "            if f'test_{feature}_rmse' in test_metrics.columns:\n",
    "                f.write(f\"* **RMSE:** {test_metrics[f'test_{feature}_rmse'].iloc[-1]:.6f}\\n\")\n",
    "            if f'test_{feature}_mae' in test_metrics.columns:\n",
    "                f.write(f\"* **MAE:** {test_metrics[f'test_{feature}_mae'].iloc[-1]:.6f}\\n\")\n",
    "            if f'test_{feature}_mape' in test_metrics.columns:\n",
    "                f.write(f\"* **MAPE:** {test_metrics[f'test_{feature}_mape'].iloc[-1]:.6f}%\\n\")\n",
    "            if f'test_{feature}_r2' in test_metrics.columns:\n",
    "                f.write(f\"* **R²:** {test_metrics[f'test_{feature}_r2'].iloc[-1]:.6f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Add prediction plots\n",
    "        f.write(\"## Prediction Visualizations\\n\\n\")\n",
    "        for feature in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "            f.write(f\"### {feature} Predictions\\n\\n\")\n",
    "            f.write(f\"![{feature} Predictions](plots/{feature}_predictions.svg)\\n\\n\")\n",
    "        \n",
    "        f.write(\"### All Features (Sample 1)\\n\\n\")\n",
    "        f.write(f\"![All Features](plots/all_features_predictions.svg)\\n\\n\")\n",
    "\n",
    "# Create the performance report\n",
    "create_performance_report(ckpt_dir=parameters['ckpt_dir'], model_name=\"mamba\")\n",
    "print(f\"Test performance report saved to {parameters['ckpt_dir']}/test_performance_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b6867",
   "metadata": {},
   "source": [
    "![Mamba Predictions](../assets/images/mamba_all_features_predictions.svg)\n",
    "\n",
    "Let's take a look at how the model performs differently across features. The metrics table shows:\n",
    "\n",
    "Close price prediction: R² of 0.99\n",
    "Open price prediction: R² of 0.99\n",
    "High price prediction: R² of 0.99\n",
    "Low price prediction: R² of 0.99\n",
    "Volume prediction: R² of only 0.17\n",
    "\n",
    "Volume is clearly the most challenging feature to predict, with an R² value of only 0.17 and extremely high MAPE of 330%. The high MAPE value indicates substantial percentage errors in predictions. This happens when:\n",
    "\n",
    "- Working with normalized data where values are close to zero\n",
    "- Dealing with highly volatile series like cryptocurrency prices\n",
    "- The model struggles with abrupt changes\n",
    "\n",
    "We can improve the model in many ways, such as:\n",
    "- Data transformation: Better normalization or scaling\n",
    "- Feature engineering: Adding more features or using different features such as technical indicators or more complex features such as wavelet transforms.\n",
    "- Model architecture: Using more complex architectures such as CNNs or LSTMs or alternative attention architectures such as state space models.\n",
    "- Hyperparameter tuning: Tuning the hyperparameters of the model to improve performance\n",
    "- Regularization: Adding regularization to the model to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49cc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
