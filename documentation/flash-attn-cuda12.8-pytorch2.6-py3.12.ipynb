{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9900fd",
   "metadata": {},
   "source": [
    "# Flash Attention with Docker for Local Development and Scaling\n",
    "\n",
    "This document provides a guide on how to set up Flash Attention using Docker for local development and scaling. It includes instructions for building the Docker image, running the container, and using Flash Attention in your projects.\n",
    "\n",
    "## Prerequisites\n",
    "- Docker installed on your machine\n",
    "- A compatible GPU (NVIDIA) \n",
    "- NVIDIA Docker runtime installed\n",
    "\n",
    "## Sections\n",
    "- [Building the Docker Image](#building-the-docker-image)\n",
    "- [Using Flash Attention Locally](#using-flash-attention)\n",
    "- [Scaling with GCP Cloud Computing](#scaling-with-gcp-cloud-computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000cc18d",
   "metadata": {},
   "source": [
    "## Building the Docker Image\n",
    "\n",
    "To build the Docker image for Flash Attention, follow these steps.\n",
    "\n",
    "1. Clone the repository:\n",
    "   ```bash\n",
    "   git clone https://github.com/gabenavarro/MLContainerLab.git\n",
    "   cd MLContainerLab\n",
    "   ```\n",
    "2. Build the Docker image:\n",
    "   ```bashll\n",
    "   docker build -f ./assets/build/Dockerfile.flashattn.cu128py26cp312 -t flash-attention:128-26-312 .\n",
    "   ```\n",
    "\n",
    "   > Note: The following steps are to development within the container. My tutorials will be run inside the container, so you can skip them if you are not interested in development.\n",
    "\n",
    "3. Run the Docker container detached with terminal access and GPUs connected:\n",
    "   ```bash\n",
    "   docker run -dt \\\n",
    "   --gpus all \\\n",
    "   -v \"$(pwd):/workspace\" \\\n",
    "   --name flash-attention \\\n",
    "   --env NVIDIA_VISIBLE_DEVICES=all \\\n",
    "   --env GOOGLE_APPLICATION_CREDENTIALS=/workspace/assets/secrets/gcp-key.json \\\n",
    "   --env SYNAPSE_TOKEN=eyJ0eXAiOiJKV1QiLCJraWQiOiJXN05OOldMSlQ6SjVSSzpMN1RMOlQ3TDc6M1ZYNjpKRU9VOjY0NFI6VTNJWDo1S1oyOjdaQ0s6RlBUSCIsImFsZyI6IlJTMjU2In0.eyJhY2Nlc3MiOnsic2NvcGUiOlsidmlldyIsImRvd25sb2FkIiwibW9kaWZ5Il0sIm9pZGNfY2xhaW1zIjp7fX0sInRva2VuX3R5cGUiOiJQRVJTT05BTF9BQ0NFU1NfVE9LRU4iLCJpc3MiOiJodHRwczovL3JlcG8tcHJvZC5wcm9kLnNhZ2ViYXNlLm9yZy9hdXRoL3YxIiwiYXVkIjoiMCIsIm5iZiI6MTc0NjE2NDc3MCwiaWF0IjoxNzQ2MTY0NzcwLCJqdGkiOiIxOTc3OSIsInN1YiI6IjM1NDE2NzQifQ.XyBybxisMzD6pUae41cuWePDuFN9GTJq2lDPOHCkVQCPXAWxQ5lsY3jBer8ACp85FP9OhX-ZiF3Zp2W5MOXWgN4PaFDHQMPvYmnuRgPINYSZd7RvAI3mKkeOAujD_p7KJBxeNdBPPVUb_V50TDI4RQ4xPknfFS8lWFgkl2WQHftbToQ6ItoTFqr7YEn6h68Og1cijYN9d7vbkLQMnwbD1FeakW7mTcfvIZfzsmpYqMhz19D8jrRTJL-7UnJitGkgAfc9gpvowUNPoZHF2eKl8a5pAOeJ6AmK1fRXhJ0kdXIoHNRrYEHR1IGJho6TioVB87TPjavaJljyLHNHK34A8w \\\n",
    "   flash-attention:128-26-312\n",
    "   ```\n",
    "   > Note: The `-v $(pwd):/workspace` option mounts the current directory to `/workspace` in the container, allowing you to access your files from within the container. <br>\n",
    "   > Note: The `--env` options set environment variables for GPU visibility and Google Cloud credentials. <br>\n",
    "   > Note: The `--gpus all` option allows the container to use all available GPUs. <br>\n",
    "   > Note: The `--name` option names the container `flash-attention`, which you can use to reference it later. <br>\n",
    "   > Note: The `-dt` option runs the container in detached mode with terminal access. <br>\n",
    "   > Note: Get your token from [Synapse](https://synapse.org/), and set it as an environment variable in the container. You can also set it in your local environment, but this is not recommended for security reasons. <br>\n",
    "   > Note: Get a GCP key from [Google Cloud](https://cloud.google.com/docs/authentication/getting-started) and set it as an environment variable in the container. You can also set it in your local environment, but this is not recommended for security reasons. <br>\n",
    "\n",
    "4. Open the container in VSCode: \n",
    "   ```bash\n",
    "   code --folder-uri vscode-remote://dev-container+flash-attention/workspace\n",
    "   ```\n",
    "\n",
    "   If you have the Remote - Containers extension installed, this command will open the current directory in VSCode, allowing you to edit files directly in the container.\n",
    "   If this fails, you can use the GUI to open the container:\n",
    "   - Open VSCode\n",
    "   - Press `F1` and type `Remote-Containers: Attach to Running Container...`\n",
    "   - Select the `flash-attention` container from the list\n",
    "   - This will open the container in a new VSCode window\n",
    "   - Set workspace to `/workspace` in the container\n",
    "\n",
    "   > Note: This command opens the current directory in VSCode, allowing you to edit files directly in the container. <br>\n",
    "   > Note: You may need to install the Remote - Containers extension in VSCode to use this feature. <br>\n",
    "   > Note: You may need to install the Python extension in VSCode to use this feature. <br>\n",
    "   > Note: You may need to install the Jupyter extension in VSCode to use this feature. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cb32f",
   "metadata": {},
   "source": [
    "## Using Flash Attention\n",
    "\n",
    "### Dataset Preparation\n",
    "Lets setup a simple example to run Flash Attention in the container. First lets start off by downloading a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451256c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SERIES_CSV = \"/workspace/datasets/btcusd_1-min_data.csv\"\n",
    "PROCESSED_DATA_DIR = \"/workspace/datasets/processed_timeseries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e9fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  113M  100  113M    0     0  28.8M      0  0:00:03  0:00:03 --:--:-- 36.0M\n",
      "Archive:  /workspace/datasets/bitcoin-historical-data.zip\n",
      "  inflating: /workspace/datasets/btcusd_1-min_data.csv  \n",
      "rm: cannot remove '\\': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Download and unzip the Bitcoin historical data dataset from Kaggle\n",
    "!curl -L -o /workspace/datasets/bitcoin-historical-data.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/mczielinski/bitcoin-historical-data \\\n",
    "    && unzip -o /workspace/datasets/bitcoin-historical-data.zip -d /workspace/datasets/ \\\n",
    "    && rm /workspace/datasets/bitcoin-historical-data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e083ecb",
   "metadata": {},
   "source": [
    "After downloading the dataset, we can load it into a Pandas DataFrame and take a look at the first few rows. The dataset contains historical data for Bitcoin, including the date, open, high, low, close prices, and volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c59d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:01:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:02:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:04:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:05:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Timestamp  Open  High   Low  Close  Volume                   datetime\n",
       "0  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:01:00+00:00\n",
       "1  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:02:00+00:00\n",
       "2  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:03:00+00:00\n",
       "3  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:04:00+00:00\n",
       "4  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:05:00+00:00"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.read_csv(TIME_SERIES_CSV, low_memory=False, nrows=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35edf2b",
   "metadata": {},
   "source": [
    "Now that we have the dataset, lets create a dataset and dataloader for the model using litdata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e22e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26978/1027441487.py:10: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/tmp/ipykernel_26978/1027441487.py:61: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_length = len(pd.read_csv(dataset_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create an account on https://lightning.ai/ to optimize your data faster using multiple nodes and large machines.\n",
      "Setting multiprocessing start_method to fork. Tip: Libraries relying on lock can hang with `fork`. To use `spawn` in notebooks, move your code to files and import it within the notebook.\n",
      "Storing the files under /workspace/datasets/processed_timeseries\n",
      "Setup started with fast_dev_run=False.\n",
      "Setup finished in 0.029 seconds. Found 7009037 items to process.\n",
      "Starting 4 workers with 7009037 items. The progress bar is only updated when a worker finishes.\n",
      "Workers are ready ! Starting data processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e6da55838b4abd988fd55f30edb8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/7009037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 inferred the following `['int', 'tensor', 'tensor', 'float']` data format.Rank 1 inferred the following `['int', 'tensor', 'tensor', 'float']` data format.\n",
      "\n",
      "Rank 2 inferred the following `['int', 'tensor', 'tensor', 'float']` data format.\n",
      "Rank 3 inferred the following `['int', 'tensor', 'tensor', 'float']` data format.\n",
      "Worker 0 is terminating.\n",
      "Worker 0 is done.\n",
      "Worker 1 is terminating.\n",
      "Worker 1 is done.\n",
      "Worker 2 is terminating.\n",
      "Worker 2 is done.\n",
      "Worker 3 is terminating.\n",
      "Worker 3 is done.\n",
      "Workers are finished.\n",
      "Finished data processing!\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import litdata as ld\n",
    "from typing import Dict, Any\n",
    "import torch\n",
    "\n",
    "def process_timeseries(file_path: str, sequence_length: int = 2048) -> Dict[str, Any]:\n",
    "    \"\"\"Process a timeseries CSV file into a format suitable for autoregressive modeling\"\"\"\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert datetime to a proper datetime object if it's not already\n",
    "    if 'datetime' in df.columns:\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Sort by timestamp to ensure chronological order\n",
    "    df = df.sort_values('Timestamp')\n",
    "    \n",
    "    # Select the numerical columns for prediction\n",
    "    numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Create sequences for autoregressive modeling\n",
    "    sequence_length = 2048  # Define how many previous timesteps to use\n",
    "    \n",
    "    # Function to create samples for litdata\n",
    "    def create_timeseries_sample(index: int) -> Dict[str, Any]:\n",
    "        if index < sequence_length or index >= len(df):\n",
    "            # Not enough previous data or beyond the dataset\n",
    "            return None\n",
    "            \n",
    "        # Get the sequence of previous data points\n",
    "        input_sequence = df.iloc[index-sequence_length:index][numerical_features].values\n",
    "        # Target is the next timestep's Close price\n",
    "        target = df.iloc[index]['Close']\n",
    "        \n",
    "        # Convert to appropriate tensor formats\n",
    "        input_tensor = torch.tensor(input_sequence, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "        \n",
    "        timestamp = df.iloc[index]['Timestamp']\n",
    "        \n",
    "        return {\n",
    "            \"index\": index,\n",
    "            \"inputs\": input_tensor,\n",
    "            \"target\": target_tensor,\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "    \n",
    "    return create_timeseries_sample\n",
    "\n",
    "# Set the sequence length for your model\n",
    "sequence_length = 2048\n",
    "\n",
    "# Get the processing function configured for your specific file\n",
    "process_function = process_timeseries(TIME_SERIES_CSV, sequence_length)\n",
    "\n",
    "# Determine the number of valid samples (will depend on your data size)\n",
    "df_length = len(pd.read_csv(TIME_SERIES_CSV))\n",
    "valid_indices = list(range(sequence_length, df_length))  # Assuming sequence_length=2048\n",
    "\n",
    "# The optimize function writes data in an optimized format\n",
    "ld.optimize(\n",
    "    fn=process_function,              # the function that processes each sample\n",
    "    inputs=valid_indices,             # the indices of valid samples\n",
    "    output_dir=PROCESSED_DATA_DIR,    # optimized data is stored here\n",
    "    num_workers=4,                    # The number of workers on the same machine\n",
    "    chunk_bytes=\"64MB\"                # size of each chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621a429",
   "metadata": {},
   "source": [
    "Next, lets split the dataset into training and validation sets. We will use 80% of the data for training and 10% for validation and 10% for test. We will also create a dataloader for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108fa799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7009037\n",
      "Train  <litdata.streaming.dataset.StreamingDataset object at 0x7f5b78db5dc0>\n",
      "Validation  <litdata.streaming.dataset.StreamingDataset object at 0x7f5b79363620>\n",
      "Test <litdata.streaming.dataset.StreamingDataset object at 0x7f5b78ede840>\n"
     ]
    }
   ],
   "source": [
    "from litdata import StreamingDataset, train_test_split\n",
    "streaming_dataset = StreamingDataset(PROCESSED_DATA_DIR) # data are stored in the cloud\n",
    "\n",
    "print(len(streaming_dataset)) # display the length of your data\n",
    "# out: 100,000\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = train_test_split(streaming_dataset, splits=[0.8, 0.1, 0.1])\n",
    "\n",
    "print(\"Train \",train_dataset)\n",
    "# out: 80,000\n",
    "\n",
    "print(\"Validation \",val_dataset)\n",
    "# out: 10,000\n",
    "\n",
    "print(\"Test\" ,test_dataset)\n",
    "# out: 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03fb99",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "Now that we have the dataset and dataloader, we can define the model. To do this, we will create a class that inherits from `nn.Module` and define the model architecture in the `__init__` method. We will also define the forward pass in the `forward` method.\n",
    "\n",
    "The model architecture consists of an embedding layer, a transformer encoder, and a linear layer. The embedding layer converts the input data into a higher-dimensional space, the transformer encoder processes the data using self-attention mechanisms, and the linear layer outputs the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c990221e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dropout_layer_norm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Transformer Layer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmha\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MHA\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrms_norm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMSNorm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfused_dense\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FusedMLP\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, optim, Tensor\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/flash_attn/ops/rms_norm.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_norm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     DropoutAddLayerNormFn,\n\u001b[1;32m      9\u001b[0m     DropoutAddLayerNormParallelResidualFn,\n\u001b[1;32m     10\u001b[0m     DropoutAddLayerNormSubsetFn,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrms_norm\u001b[39m(x, weight, epsilon):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DropoutAddLayerNormFn\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     16\u001b[0m         x, \u001b[38;5;28;01mNone\u001b[39;00m, weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0.0\u001b[39m, epsilon, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/flash_attn/ops/layer_norm.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2022, Tri Dao.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Adapted from https://github.com/NVIDIA/apex/blob/master/apex/contrib/layer_norm/layer_norm.py\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdropout_layer_norm\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dropout_layer_norm'"
     ]
    }
   ],
   "source": [
    "# Transformer Layer\n",
    "from flash_attn.modules.mha import MHA\n",
    "from flash_attn.ops.rms_norm import RMSNorm\n",
    "from flash_attn.ops.fused_dense import FusedMLP\n",
    "from torch import nn, optim, Tensor\n",
    "import lightning as pl\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            layer_idx: int,\n",
    "            embed_dim: int,\n",
    "            num_heads: int,\n",
    "            mlp_ratio: float = 4.0,\n",
    "            proj_groups: int = 1,\n",
    "            fast_attention: bool = True,\n",
    "        ):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.attention = MHA(\n",
    "            D = embed_dim,                      # Dimension of the model\n",
    "            num_heads = num_heads,              # Number of attention heads\n",
    "            causal = True,                      # Causal attention\n",
    "            layer_idx = layer_idx,              # Layer index for rotary embedding\n",
    "            num_heads_kv = num_heads // proj_groups, # Number of heads for key/value\n",
    "            rotary_emb_dim = embed_dim // num_heads, # Rotary embedding dimension\n",
    "            use_flash_attn = fast_attention,         # Use flash attention\n",
    "            return_residual = False                  # Return residual connection\n",
    "\n",
    "        )\n",
    "        self.norm1 = RMSNorm(embed_dim)\n",
    "        self.mlp = FusedMLP(embed_dim, int(embed_dim * mlp_ratio))\n",
    "        self.norm2 = RMSNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        attn_output = self.attention(x)\n",
    "        x = x + self.norm1(attn_output)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + self.norm2(mlp_output)\n",
    "        return x\n",
    "    \n",
    "# Transformer Model\n",
    "class TransformerModel(pl.LightningModule):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, num_layers: int, mlp_ratio: float = 4.0):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(layer_idx, embed_dim, num_heads, mlp_ratio) for layer_idx in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_dim, 1)  # Output layer for regression\n",
    "\n",
    "    # In pl.LightningModule, the forward method is used for inference\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.fc_out(x)\n",
    "    \n",
    "    # Training step is where the model learns from the data\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs = batch['inputs']\n",
    "        targets = batch['target']\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(outputs, targets)\n",
    "        \n",
    "        # Log the loss for monitoring\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    # Validation step is where the model is evaluated on the validation set\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs = batch['inputs']\n",
    "        targets = batch['target']\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(outputs, targets)\n",
    "        \n",
    "        # Log the loss for monitoring\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    # Configuring the optimizer\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "# Initialize the model\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "model = TransformerModel(embed_dim, num_heads, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb87e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
