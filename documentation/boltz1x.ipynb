{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a3bdf6",
   "metadata": {},
   "source": [
    "# Botlz-1x with Docker for Local Development and Cloud Deployment\n",
    "\n",
    "---\n",
    "\n",
    "This documentation provides a guide on Botlz-1x implemented in Python, designed for both local development and cloud deployment using Docker. It covers the following topics:\n",
    "\n",
    "1. **Introduction to Botlz-1x**: Overview of the Botlz-1x and its applications.\n",
    "2. **Setting Up the Development Environment**: Step-by-step instructions for setting up a local development environment using Docker.\n",
    "3. **Building and Running the Docker Container**: Instructions for building the Docker image and running the container.\n",
    "4. **Deploying to the Cloud**: Guidelines for deploying the Botlz-1x to a cloud platform using Docker.\n",
    "5. **Best Practices**: Tips and best practices for working with Botlz-1x and Docker.\n",
    "\n",
    "## Introduction to Botlz-1x and the Protein-Folding Journey\n",
    "\n",
    "---\n",
    "\n",
    "Predicting a protein’s three-dimensional shape from its amino acid sequence has been a “grand challenge” in biology for over half a century. In 1973, Christian Anfinsen demonstrated that the information for folding is encoded in the sequence itself when his chemically denatured ribonuclease spontaneously refolded into its active conformation upon removal of denaturants ([Aklectures][1]). This foundational experiment launched an era of **physics-based** and **statistical** modeling.\n",
    "\n",
    "In 1994, the biannual **CASP** (Critical Assessment of Structure Prediction) competition was created to objectively benchmark methods—even before structures were publicly released ([Wikipedia][2]). Early efforts like **Rosetta** (first in FORTRAN, then C++) applied fragment assembly and Monte Carlo sampling for **de novo** prediction, winning CASP tasks via clever energy functions and design protocols ([docs.rosettacommons.org][3], [PMC][4]). Over the 2000s, Rosetta expanded into docking, design, and community-driven platforms like Foldit ([Wikipedia][5]).\n",
    "\n",
    "The deep-learning era arrived in 2020 when **DeepMind’s AlphaFold 2** achieved atomic-level accuracy in CASP14, effectively solving the prediction problem for most single-chain proteins ([Nature][6], [WIRED][7]). Soon after, the Baker lab released **RoseTTAFold**, democratizing high-accuracy predictions on consumer GPUs in minutes ([Baker Lab][8]). In 2024, the Nobel Prize for Chemistry recognized Demis Hassabis, John Jumper, and David Baker for these complementary breakthroughs in AI-driven folding and design ([Le Monde.fr][9]).\n",
    "\n",
    "Building on this lineage, **Botlz-1x** leverages a novel **Boltzmann-inspired** architecture that blends state-space models with graph-based potentials to predict structures faster and with fewer resources. This notebook shows you how to:\n",
    "\n",
    "1. **Containerize** Botlz-1x in Docker for reproducible local experiments\n",
    "2. **Scale** training and inference via cloud deployment\n",
    "3. **Integrate** with existing folding pipelines and compare performance\n",
    "\n",
    "Whether you’re an academic exploring protein design or an industry practitioner deploying at scale, Botlz-1x offers a lightweight, production-ready alternative in the post-AlphaFold landscape.\n",
    "\n",
    "\n",
    "## Notebook Roadmap\n",
    "\n",
    "---\n",
    "\n",
    "### Sections\n",
    "- [Building and Running the Docker Container](#building-and-running-the-docker-container)\n",
    "- [Using Botlz-1x](#using-botlz-1x)\n",
    "- [Deploying to the Cloud](#deploying-to-the-cloud)\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following installed on your local machine:\n",
    "\n",
    "- Docker: [Install Docker](https://docs.docker.com/get-docker/)\n",
    "- A compatible GPU (for Botlz-1x)\n",
    "- NVIDIA drivers (if using GPU)\n",
    "\n",
    "\n",
    "\n",
    "## Building and Running the Docker Container\n",
    "\n",
    "---\n",
    "\n",
    "To build and run the Docker container for Botlz-1x, follow these steps:\n",
    "\n",
    "1. **Clone the Repository**: Clone the Botlz-1x repository to your local machine.\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/gabenavarro/MLContainerLab.git\n",
    "   cd MLContainerLab\n",
    "   ```\n",
    "\n",
    "2. **Build the Docker Image**: Use the provided Dockerfile to build the Docker image.\n",
    "\n",
    "   ```bash\n",
    "   # You can choose any tag you want for the image\n",
    "   # Feel free to play around with the base image, just make sure the host has the same or higher CUDA version\n",
    "   docker build -f ./assets/build/Dockerfile.boltz1x.cu126cp310 -t boltz1x:126-310 .\n",
    "   ```\n",
    "3. **Run the Docker Container**: Run the Docker container with the necessary configurations. In the first example, we will run the container locally with GPU support. This is the recommended way to run a container while in development mode. For scaling up, we will use the second example which runs the container in the cloud.\n",
    "\n",
    "   ```bash\n",
    "    # Run the container with GPU support\n",
    "    docker run -dt \\\n",
    "        --gpus all \\\n",
    "        --shm-size=64g \\\n",
    "        -v \"$(pwd):/workspace\" \\\n",
    "        --name boltz1x \\\n",
    "        --env NVIDIA_VISIBLE_DEVICES=all \\\n",
    "        --env GOOGLE_APPLICATION_CREDENTIALS=/workspace/assets/secrets/gcp-key.json \\\n",
    "        boltz1x:126-310\n",
    "    ```\n",
    "> Note: The `-v \"$(pwd):/workspace\"` option mounts the current directory to `/workspace` in the container, allowing you to access your local files from within the container. The `--env` options set environment variables for GPU visibility and Google Cloud credentials.<br>\n",
    "> Note: The `--gpus all` option allows the container to use all available GPUs. <br>\n",
    "\n",
    "4. **Access the Container with IDE**: In this example, we will use Visual Studio Code to access the container. You can use any IDE of your choice.\n",
    "\n",
    "   ```bash\n",
    "   # In a scriptable manner\n",
    "   CONTAINER_NAME=boltz1x\n",
    "   FOLDER=/workspace\n",
    "   HEX_CONFIG=$(printf {\\\"containerName\\\":\\\"/$CONTAINER_NAME\\\"} | od -A n -t x1 | tr -d '[\\n\\t ]')\n",
    "   code --folder-uri \"vscode-remote://attached-container+$HEX_CONFIG$FOLDER\"\n",
    "   ```\n",
    "\n",
    "> Note: The `code` command is used to open Visual Studio Code. Make sure you have the Remote - Containers extension installed in VS Code to access the container directly. <br>\n",
    "> Note: Make sure you have installed Remote - Containers extension in VS Code.<br>\n",
    "\n",
    "\n",
    "\n",
    "Quick use\n",
    "\n",
    "```bash\n",
    "  --out_dir PATH               The path where to save the predictions.\n",
    "  --cache PATH                 The directory where to download the data and\n",
    "                               model. Default is ~/.boltz, or $BOLTZ_CACHE if\n",
    "                               set.\n",
    "  --checkpoint PATH            An optional checkpoint, will use the provided\n",
    "                               Boltz-1 model by default.\n",
    "  --devices INTEGER            The number of devices to use for prediction.\n",
    "                               Default is 1.\n",
    "  --accelerator [gpu|cpu|tpu]  The accelerator to use for prediction. Default\n",
    "                               is gpu.\n",
    "  --recycling_steps INTEGER    The number of recycling steps to use for\n",
    "                               prediction. Default is 3.\n",
    "  --sampling_steps INTEGER     The number of sampling steps to use for\n",
    "                               prediction. Default is 200.\n",
    "  --diffusion_samples INTEGER  The number of diffusion samples to use for\n",
    "                               prediction. Default is 1.\n",
    "  --step_scale FLOAT           The step size is related to the temperature at\n",
    "                               which the diffusion process samples the\n",
    "                               distribution.The lower the higher the diversity\n",
    "                               among samples (recommended between 1 and 2).\n",
    "                               Default is 1.638.\n",
    "  --write_full_pae             Whether to dump the pae into a npz file.\n",
    "                               Default is True.\n",
    "  --write_full_pde             Whether to dump the pde into a npz file.\n",
    "                               Default is False.\n",
    "  --output_format [pdb|mmcif]  The output format to use for the predictions.\n",
    "                               Default is mmcif.\n",
    "  --num_workers INTEGER        The number of dataloader workers to use for\n",
    "                               prediction. Default is 2.\n",
    "  --override                   Whether to override existing found predictions.\n",
    "                               Default is False.\n",
    "  --seed INTEGER               Seed to use for random number generator.\n",
    "                               Default is None (no seeding).\n",
    "  --use_msa_server             Whether to use the MMSeqs2 server for MSA\n",
    "                               generation. Default is False.\n",
    "  --msa_server_url TEXT        MSA server url. Used only if --use_msa_server\n",
    "                               is set.\n",
    "  --msa_pairing_strategy TEXT  Pairing strategy to use. Used only if\n",
    "                               --use_msa_server is set. Options are 'greedy'\n",
    "                               and 'complete'\n",
    "  --no_potentials              Whether to not use potentials for steering.\n",
    "                               Default is False.\n",
    "```\n",
    "\n",
    "\n",
    "[1]: https://aklectures.com/lecture/structure-of-proteins/anfinsens-experiment-of-protein-folding \"Anfinsen's Experiment of Protein Folding - AK Lectures\"\n",
    "[2]: https://en.wikipedia.org/wiki/CASP \"CASP - Wikipedia\"\n",
    "[3]: https://docs.rosettacommons.org/docs/latest/meta/Rosetta-Timeline \"History of Rosetta\"\n",
    "[4]: https://pmc.ncbi.nlm.nih.gov/articles/PMC7603796 \"Macromolecular modeling and design in Rosetta: recent methods ...\"\n",
    "[5]: https://en.wikipedia.org/wiki/Rosetta%40home \"Rosetta@home\"\n",
    "[6]: https://www.nature.com/articles/s41586-021-03819-2 \"Highly accurate protein structure prediction with AlphaFold - Nature\"\n",
    "[7]: https://www.wired.com/story/deepmind-alphafold-protein-diseases \"DeepMind wants to use its AI to cure neglected diseases\"\n",
    "[8]: https://www.bakerlab.org/2021/07/15/accurate-protein-structure-prediction-accessible \"Accurate protein structure prediction accessible to all - Baker Lab\"\n",
    "[9]: https://www.lemonde.fr/en/science/article/2024/10/09/nobel-prize-for-chemistry-2024-artificial-intelligence-garners-more-recognition_6728828_10.html \"Nobel Prize for Chemistry 2024: Artificial intelligence garners more recognition\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056dba5",
   "metadata": {},
   "source": [
    "## Using Botlz-1x\n",
    "\n",
    "Now we will go ahead and run Boltz-1x with a few different file formats in order to understand the different configurations.  First we will start with a fasta file.\n",
    "\n",
    "```fasta\n",
    ">A|protein|./examples/msa/seq1.a3m\n",
    "MVTPEGNVSLVDESLLVGVTDEDRAVRSAHQFYERLIGLWAPAVMEAAHELGV\n",
    ">B|protein|./examples/msa/seq1.a3m\n",
    "MVTPEGNVSLVDESLLVGVTDEDRAVRSAHQFYERLIGLWAPAVMEAAHELGV\n",
    ">C|ccd\n",
    "SAH\n",
    ">D|ccd\n",
    "SAH\n",
    ">E|smiles\n",
    "N[C@@H](Cc1ccc(O)cc1)C(=O)O\n",
    ">F|smiles\n",
    "N[C@@H](Cc1ccc(O)cc1)C(=O)O\n",
    "```\n",
    "\n",
    "The header is separated by `|` character. The first item is the chain ID and must be unique. The second item is the entity type, with options `protein`, `dna`, `rna`, `ccd`, and `smiles`. The last index is the path to a precomputed MSA file, although this is optional as the MSA file can be calculated as part of the boltz run. In a production environment, it makes sense to first pre-compute all MSA files using a CPU and memory high box, then run protein folding inference with a GPU heavy box.\n",
    "\n",
    "The exact fast file we will use is found [boltz1x.fasta](../assets/test-files/boltz1.fasta). As has the following content:\n",
    "\n",
    "```fasta\n",
    ">A|protein\n",
    "DEAIHCPPCSEEKLARCRPPVGCEELVREPGCGCCATCALGLGMPCGVYTPRCGSGLRCYPPRGVEKPLHTLMHGQGVCMELAEIEAIQESL\n",
    ">B|protein\n",
    "GPETLCGAELVDALQFVCGDRGFYFNKPTGYGSSSRRAPQTGIVDECCFRSCDLRRLEMYCAPLKPAKSA\n",
    "```\n",
    "\n",
    "This the PDB file [2DSP](https://www.rcsb.org/structure/2DSP). \n",
    "\n",
    "![image](../assets/images/2dsp_assembly-1.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3442332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input data.\n",
      "Running predictions for 1 structure\n",
      "Processing input data.\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]Generating MSA for /workspace/assets/test-files/boltz1.fasta with 2 protein entities.\n",
      "\n",
      "  0%|                                      | 0/300 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "SUBMIT:   0%|                              | 0/300 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "PENDING:   0%|                             | 0/300 [elapsed: 00:00 remaining: ?]\u001b[ASleeping for 5s. Reason: PENDING\n",
      "\n",
      "RUNNING:   0%|                             | 0/300 [elapsed: 00:06 remaining: ?]\u001b[A\n",
      "RUNNING:   2%|▍                        | 5/300 [elapsed: 00:06 remaining: 06:02]\u001b[ASleeping for 10s. Reason: RUNNING\n",
      "\n",
      "RUNNING:   2%|▍                        | 5/300 [elapsed: 00:16 remaining: 06:02]\u001b[A\n",
      "RUNNING:   5%|█▏                      | 15/300 [elapsed: 00:16 remaining: 05:10]\u001b[ASleeping for 7s. Reason: RUNNING\n",
      "\n",
      "RUNNING:   5%|█▏                      | 15/300 [elapsed: 00:23 remaining: 05:10]\u001b[A\n",
      "RUNNING:   7%|█▊                      | 22/300 [elapsed: 00:23 remaining: 04:57]\u001b[ASleeping for 7s. Reason: RUNNING\n",
      "\n",
      "COMPLETE:   7%|█▋                     | 22/300 [elapsed: 00:32 remaining: 04:57]\u001b[A\n",
      "COMPLETE: 100%|██████████████████████| 300/300 [elapsed: 00:32 remaining: 00:00]\u001b[A\n",
      "\n",
      "  0%|                                      | 0/300 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "SUBMIT:   0%|                              | 0/300 [elapsed: 00:00 remaining: ?]\u001b[A\n",
      "PENDING:   0%|                             | 0/300 [elapsed: 00:00 remaining: ?]\u001b[ASleeping for 8s. Reason: PENDING\n",
      "\n",
      "RUNNING:   0%|                             | 0/300 [elapsed: 00:08 remaining: ?]\u001b[A\n",
      "RUNNING:   3%|▋                        | 8/300 [elapsed: 00:08 remaining: 05:17]\u001b[ASleeping for 8s. Reason: RUNNING\n",
      "\n",
      "RUNNING:   3%|▋                        | 8/300 [elapsed: 00:17 remaining: 05:17]\u001b[A\n",
      "RUNNING:   5%|█▎                      | 16/300 [elapsed: 00:17 remaining: 05:02]\u001b[ASleeping for 5s. Reason: RUNNING\n",
      "\n",
      "RUNNING:   5%|█▎                      | 16/300 [elapsed: 00:22 remaining: 05:02]\u001b[A\n",
      "RUNNING:   7%|█▋                      | 21/300 [elapsed: 00:22 remaining: 05:00]\u001b[ASleeping for 9s. Reason: RUNNING\n",
      "\n",
      "RUNNING:   7%|█▋                      | 21/300 [elapsed: 00:33 remaining: 05:00]\u001b[A\n",
      "RUNNING:  10%|██▍                     | 30/300 [elapsed: 00:33 remaining: 05:04]\u001b[ASleeping for 7s. Reason: RUNNING\n",
      "\n",
      "RUNNING:  10%|██▍                     | 30/300 [elapsed: 00:40 remaining: 05:04]\u001b[A\n",
      "RUNNING:  12%|██▉                     | 37/300 [elapsed: 00:40 remaining: 04:49]\u001b[ASleeping for 7s. Reason: RUNNING\n",
      "\n",
      "COMPLETE:  12%|██▊                    | 37/300 [elapsed: 00:47 remaining: 04:49]\u001b[A\n",
      "COMPLETE: 100%|██████████████████████| 300/300 [elapsed: 00:48 remaining: 00:00]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 1/1 [01:21<00:00, 81.73s/it]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|████████████████████| 1/1 [09:34<00:00,  0.00it/s]Number of failed examples: 0\n",
      "Predicting DataLoader 0: 100%|████████████████████| 1/1 [09:34<00:00,  0.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Simple example\n",
    "!boltz predict /workspace/assets/test-files/boltz1.fasta \\\n",
    "    --recycling_steps 10 \\\n",
    "    --diffusion_samples 25 \\\n",
    "    --accelerator gpu \\\n",
    "    --out_dir /workspace/datasets/boltz1x/predict2 \\\n",
    "    --cache /workspace/datasets/boltz1x/cache \\\n",
    "    --use_msa_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffa86f",
   "metadata": {},
   "source": [
    "The results are fairly close to experimental modal.\n",
    "![image](../assets/images/bolt_prediction_igf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ac4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
