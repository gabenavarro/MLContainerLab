{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9900fd",
   "metadata": {},
   "source": [
    "# Flash Attention with Docker for Local Development and Scaling\n",
    "\n",
    "This document provides a guide on how to set up Flash Attention using Docker for local development and scaling. It includes instructions for building the Docker image, running the container, and using Flash Attention in your projects.\n",
    "\n",
    "## Prerequisites\n",
    "- Docker installed on your machine\n",
    "- A compatible GPU (NVIDIA) \n",
    "- NVIDIA Docker runtime installed\n",
    "\n",
    "## Sections\n",
    "- [Building the Docker Image](#building-the-docker-image)\n",
    "- [Using Flash Attention Locally](#using-flash-attention)\n",
    "- [Scaling with GCP Cloud Computing](#scaling-with-gcp-cloud-computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000cc18d",
   "metadata": {},
   "source": [
    "## Building the Docker Image\n",
    "\n",
    "To build the Docker image for Flash Attention, follow these steps.\n",
    "\n",
    "1. Clone the repository:\n",
    "   ```bash\n",
    "   git clone https://github.com/gabenavarro/MLContainerLab.git\n",
    "   cd MLContainerLab\n",
    "   ```\n",
    "2. Build the Docker image:\n",
    "   ```bashll\n",
    "   docker build -f ./assets/build/Dockerfile.flashattn.cu128py26cp312 -t flash-attention:128-26-312 .\n",
    "   ```\n",
    "\n",
    "   > Note: The following steps are to development within the container. My tutorials will be run inside the container, so you can skip them if you are not interested in development.\n",
    "\n",
    "3. Run the Docker container detached with terminal access and GPUs connected:\n",
    "   ```bash\n",
    "      docker run -dt \\\n",
    "         --gpus all \\\n",
    "         -v \"$(pwd):/workspace\" \\\n",
    "         --name flash-attention \\\n",
    "         --env NVIDIA_VISIBLE_DEVICES=all \\\n",
    "         --env GOOGLE_APPLICATION_CREDENTIALS=/workspace/assets/secrets/gcp-key.json \\\n",
    "         flash-attention:128-26-312\n",
    "   ```\n",
    "   > Note: The `-v $(pwd):/workspace` option mounts the current directory to `/workspace` in the container, allowing you to access your files from within the container. <br>\n",
    "   > Note: The `--env` options set environment variables for GPU visibility and Google Cloud credentials. <br>\n",
    "   > Note: The `--gpus all` option allows the container to use all available GPUs. <br>\n",
    "   > Note: The `--name` option names the container `flash-attention`, which you can use to reference it later. <br>\n",
    "   > Note: The `-dt` option runs the container in detached mode with terminal access. <br>\n",
    "   > Note: Get your token from [Synapse](https://synapse.org/), and set it as an environment variable in the container. You can also set it in your local environment, but this is not recommended for security reasons. <br>\n",
    "   > Note: Get a GCP key from [Google Cloud](https://cloud.google.com/docs/authentication/getting-started) and set it as an environment variable in the container. You can also set it in your local environment, but this is not recommended for security reasons. <br>\n",
    "\n",
    "4. Open the container in VSCode: \n",
    "   ```bash\n",
    "   # In a scriptable manner\n",
    "   CONTAINER_NAME=flash-attention\n",
    "   FOLDER=/workspace\n",
    "   HEX_CONFIG=$(printf {\\\"containerName\\\":\\\"/$CONTAINER_NAME\\\"} | od -A n -t x1 | tr -d '[\\n\\t ]')\n",
    "   code --folder-uri \"vscode-remote://attached-container+$HEX_CONFIG$FOLDER\"\n",
    "   ```\n",
    "\n",
    "   If you have the Remote - Containers extension installed, this command will open the current directory in VSCode, allowing you to edit files directly in the container.\n",
    "   If this fails, you can use the GUI to open the container:\n",
    "   - Open VSCode\n",
    "   - Press `F1` and type `Remote-Containers: Attach to Running Container...`\n",
    "   - Select the `flash-attention` container from the list\n",
    "   - This will open the container in a new VSCode window\n",
    "   - Set workspace to `/workspace` in the container\n",
    "\n",
    "   > Note: This command opens the current directory in VSCode, allowing you to edit files directly in the container. <br>\n",
    "   > Note: You may need to install the Remote - Containers extension in VSCode to use this feature. <br>\n",
    "   > Note: You may need to install the Python extension in VSCode to use this feature. <br>\n",
    "   > Note: You may need to install the Jupyter extension in VSCode to use this feature. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cb32f",
   "metadata": {},
   "source": [
    "## Using Flash Attention\n",
    "\n",
    "### Dataset Preparation\n",
    "Lets setup a simple example to run Flash Attention in the container. First lets start off by downloading a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download and unzip the Bitcoin historical data dataset from Kaggle\n",
    "!curl -L -o /workspace/datasets/bitcoin-historical-data.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/mczielinski/bitcoin-historical-data \\\n",
    "    && unzip -o /workspace/datasets/bitcoin-historical-data.zip -d /workspace/datasets/ \\\n",
    "    && rm /workspace/datasets/bitcoin-historical-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451256c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SERIES_CSV = \"/workspace/datasets/btcusd_1-min_data.csv\"\n",
    "PROCESSED_DATA_DIR = \"/workspace/datasets/auto_regressive_processed_timeseries\"\n",
    "CKPT_DIR = \"/workspace/datasets/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6838fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories if they do not exist\n",
    "import os\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e083ecb",
   "metadata": {},
   "source": [
    "After downloading the dataset, we can load it into a Pandas DataFrame and take a look at the first few rows. The dataset contains historical data for Bitcoin, including the date, open, high, low, close prices, and volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10c59d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:01:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:02:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:04:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.325412e+09</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01 10:05:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Timestamp  Open  High   Low  Close  Volume                   datetime\n",
       "0  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:01:00+00:00\n",
       "1  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:02:00+00:00\n",
       "2  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:03:00+00:00\n",
       "3  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:04:00+00:00\n",
       "4  1.325412e+09  4.58  4.58  4.58   4.58     0.0  2012-01-01 10:05:00+00:00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.read_csv(TIME_SERIES_CSV, low_memory=False, nrows=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35edf2b",
   "metadata": {},
   "source": [
    "Now that we have the dataset, lets create a dataset and dataloader for the model using litdata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ccd518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create an account on https://lightning.ai/ to optimize your data faster using multiple nodes and large machines.\n",
      "Setting multiprocessing start_method to fork. Tip: Libraries relying on lock can hang with `fork`. To use `spawn` in notebooks, move your code to files and import it within the notebook.\n",
      "Storing the files under /workspace/datasets/auto_regressive_processed_timeseries\n",
      "Setup started with fast_dev_run=False.\n",
      "Setup finished in 0.003 seconds. Found 13690 items to process.\n",
      "Starting 4 workers with 13690 items. The progress bar is only updated when a worker finishes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3577: UserWarning: A newer version of litdata is available (0.2.46). Please consider upgrading with `pip install -U litdata`. Not all functionalities of the platform can be guaranteed to work with the current version.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 inferred the following `['int', 'numpy', 'no_header_numpy:13', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'str']` data format.\n",
      "Rank 1 inferred the following `['int', 'numpy', 'no_header_numpy:13', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'str']` data format.\n",
      "Workers are ready ! Starting data processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fc826be02b4d58831b5ae88f1af91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/13690 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 2 inferred the following `['int', 'numpy', 'no_header_numpy:13', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'str']` data format.\n",
      "Rank 3 inferred the following `['int', 'numpy', 'no_header_numpy:13', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'float', 'str']` data format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/litdata/streaming/writer.py:275: UserWarning: An item was larger than the target chunk size (64.0 MB). The current chunk will be 64.0 MB in size.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/litdata/streaming/writer.py:275: UserWarning: An item was larger than the target chunk size (64.0 MB). The current chunk will be 64.0 MB in size.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/litdata/streaming/writer.py:275: UserWarning: An item was larger than the target chunk size (64.0 MB). The current chunk will be 64.0 MB in size.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/litdata/streaming/writer.py:275: UserWarning: An item was larger than the target chunk size (64.0 MB). The current chunk will be 64.0 MB in size.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/litdata/streaming/writer.py:275: UserWarning: An item was larger than the target chunk size (64.0 MB). The current chunk will be 64.0 MB in size.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/litdata/streaming/writer.py:275: UserWarning: An item was larger than the target chunk size (64.0 MB). The current chunk will be 64.0 MB in size.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/litdata/streaming/writer.py:275: UserWarning: An item was larger than the target chunk size (64.0 MB). The current chunk will be 64.0 MB in size.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/litdata/streaming/writer.py:275: UserWarning: An item was larger than the target chunk size (64.0 MB). The current chunk will be 64.0 MB in size.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1 is terminating.\n",
      "Worker 0 is terminating.\n",
      "Worker 1 is done.\n",
      "Worker 2 is terminating.\n",
      "Worker 3 is terminating.\n",
      "Worker 0 is done.\n",
      "Worker 2 is done.Worker 3 is done.\n",
      "\n",
      "Workers are finished.\n",
      "Finished data processing!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import litdata as ld\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "def process_timeseries(file_path: str, sequence_length: int = 2048) -> Dict[str, Any]:\n",
    "    \"\"\"Process a timeseries CSV file into a format suitable for autoregressive modeling.\"\"\"\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    if \"datetime\" in df.columns:\n",
    "        df = df.drop(columns=[\"datetime\"])\n",
    "    df = df.sort_values('Timestamp')\n",
    "    numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # —— NEW: log-transform price values to handle exponential growth ——\n",
    "    for price_col in ['Open', 'High', 'Low', 'Close']:\n",
    "        # Ensure all values are positive before log transform\n",
    "        df[price_col] = np.log(df[price_col].replace([np.inf, -np.inf, 0, np.nan], 0.01).fillna(0.01))\n",
    "    \n",
    "    # —— NEW: log1p-transform volume to reduce extreme skew ——\n",
    "    df['Volume'] = np.log1p(df['Volume'].replace([np.inf, -np.inf], np.nan).fillna(0.0))\n",
    "    \n",
    "    stats = {}\n",
    "    for feature in numerical_features:\n",
    "        vals = df[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        mean, std = (vals.mean(), vals.std() or 1.0)\n",
    "        stats[feature] = {'mean': mean, 'std': std}\n",
    "        df[feature] = (df[feature] - mean) / std\n",
    "\n",
    "    if len(df) <= sequence_length:\n",
    "        print(f\"Warning: only {len(df)} rows < sequence_length={sequence_length}\")\n",
    "        return None\n",
    "\n",
    "    def create_timeseries_sample(index: int) -> Dict[str, Any]:\n",
    "        if index < sequence_length or index >= len(df):\n",
    "            return {\"index\": index,\n",
    "                    \"inputs\": np.zeros((sequence_length,5),dtype=np.float32),\n",
    "                    \"mask\": np.zeros(sequence_length, dtype=bool),\n",
    "                    \"stats\": stats}\n",
    "        seq = df.iloc[index-sequence_length:index][numerical_features].values\n",
    "        arr = np.nan_to_num(seq.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        mask = ~np.isnan(seq).any(axis=1)\n",
    "        return {\"index\": index, \"inputs\": arr, \"mask\": mask, \"stats\": stats}\n",
    "\n",
    "    # Store original stats for inverse transformation during inference\n",
    "    stats['transform_type'] = 'log_then_zscore'\n",
    "    \n",
    "    return create_timeseries_sample\n",
    "\n",
    "\n",
    "# Set the sequence length for your model\n",
    "sequence_length = 2048\n",
    "\n",
    "# Get the processing function configured for your specific file\n",
    "process_function = process_timeseries(TIME_SERIES_CSV, sequence_length)\n",
    "\n",
    "# Filter indices to exclude those with NaN values\n",
    "def get_valid_indices():\n",
    "    df = pd.read_csv(TIME_SERIES_CSV, low_memory=False)\n",
    "    if \"datetime\" in df.columns:\n",
    "        df = df.drop(columns=[\"datetime\"])\n",
    "    \n",
    "    df = df.sort_values('Timestamp')\n",
    "    numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    \n",
    "    # Replace infinity values with NaN\n",
    "    for feature in numerical_features:\n",
    "        df[feature] = df[feature].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    valid_indices = []\n",
    "    for idx in range(sequence_length, len(df), int(sequence_length * 0.25)):\n",
    "        # Check if the entire sequence has no NaN values\n",
    "        sequence = df.iloc[idx-sequence_length:idx][numerical_features].values\n",
    "        if not np.isnan(sequence).any() and not np.isinf(sequence).any():\n",
    "            valid_indices.append(idx)\n",
    "        else:\n",
    "            print(f\"Skipping index {idx} due to NaN or inf values in the sequence.\")\n",
    "    \n",
    "    if not valid_indices:\n",
    "        raise ValueError(\"No valid sequences found! All sequences contain NaN or inf values.\")\n",
    "    \n",
    "    return valid_indices\n",
    "\n",
    "valid_indices = get_valid_indices()\n",
    "\n",
    "# The optimize function writes data in an optimized format\n",
    "ld.optimize(\n",
    "    fn=process_function,              # the function that processes each sample\n",
    "    inputs=valid_indices,             # the indices of valid samples\n",
    "    output_dir=PROCESSED_DATA_DIR,    # optimized data is stored here\n",
    "    num_workers=4,                    # The number of workers on the same machine\n",
    "    chunk_bytes=\"64MB\"                # size of each chunk\n",
    ")\n",
    "# Takes about 30 seconds to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621a429",
   "metadata": {},
   "source": [
    "Next, lets split the dataset into training and validation sets. We will use 80% of the data for training and 10% for validation and 10% for test. We will also create a dataloader for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "108fa799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13690\n",
      "Train  10952\n",
      "Validation  1369\n",
      "Test  1369\n"
     ]
    }
   ],
   "source": [
    "from litdata import StreamingDataset, StreamingDataLoader, train_test_split\n",
    "import torch\n",
    "streaming_dataset = StreamingDataset(PROCESSED_DATA_DIR) # data are stored in the cloud\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Filter out None values\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        # Return empty tensors if the batch is empty\n",
    "        return {\n",
    "            \"index\": torch.tensor([], dtype=torch.long),\n",
    "            \"inputs\": torch.tensor([], dtype=torch.float32),\n",
    "            \"mask\": torch.tensor([], dtype=torch.bool),\n",
    "            \"stats\": {}\n",
    "        }\n",
    "    \n",
    "    # Process each key separately\n",
    "    indices = torch.tensor([item[\"index\"] for item in batch], dtype=torch.long)\n",
    "    \n",
    "    # Make sure arrays are writable by copying and convert to tensor\n",
    "    inputs = torch.stack([torch.tensor(np.nan_to_num(item[\"inputs\"].copy(), nan=0.0), dtype=torch.float32) for item in batch])\n",
    "    masks = torch.stack([torch.tensor(item[\"mask\"].copy(), dtype=torch.bool) for item in batch])\n",
    "    \n",
    "    # Get stats (use first non-empty item's stats)\n",
    "    stats = next((item[\"stats\"] for item in batch if \"stats\" in item), {})\n",
    "    \n",
    "    return {\n",
    "        \"index\": indices,\n",
    "        \"inputs\": inputs,\n",
    "        \"mask\": masks,\n",
    "        \"stats\": stats\n",
    "    }\n",
    "\n",
    "print(len(streaming_dataset)) # display the length of your data\n",
    "# out: 100,000\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = train_test_split(streaming_dataset, splits=[0.8, 0.1, 0.1])\n",
    "\n",
    "print(\"Train \", len(train_dataset))\n",
    "train_dataloader = StreamingDataLoader(train_dataset, num_workers=4, batch_size=32, shuffle=True, collate_fn=custom_collate)  # Create DataLoader for training\n",
    "# out: 80,000\n",
    "\n",
    "print(\"Validation \", len(val_dataset))\n",
    "val_dataloader = StreamingDataLoader(val_dataset, num_workers=4, batch_size=32, shuffle=False, collate_fn=custom_collate)  # Create DataLoader for validation\n",
    "\n",
    "test_dataloader = StreamingDataLoader(test_dataset, num_workers=4, batch_size=32, shuffle=False, collate_fn=custom_collate)\n",
    "# out: 10,000\n",
    "print(\"Test \", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe79c285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2048, 5])\n"
     ]
    }
   ],
   "source": [
    "# Get train_dataset and findout dataset shape for input dimensions\n",
    "for batch in train_dataloader:\n",
    "    print(batch['inputs'].shape)  # Check the shape of the input tensor\n",
    "    # out: torch.Size([32, 2048, 5])\n",
    "    # 32 is the batch size, 2048 is the sequence length, and 5 is the number of features\n",
    "    break  # Only need to check the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03fb99",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "Now that we have the dataset and dataloader, we can define the model. To do this, we will create a class that inherits from `nn.Module` and define the model architecture in the `__init__` method. We will also define the forward pass in the `forward` method.\n",
    "\n",
    "The model architecture consists of an embedding layer, a transformer encoder, and a linear layer. The embedding layer converts the input data into a higher-dimensional space, the transformer encoder processes the data using self-attention mechanisms, and the linear layer outputs the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c990221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/flash_attn-2.7.4.post1-py3.12-linux-x86_64.egg/flash_attn/ops/fused_dense.py:29: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/usr/local/lib/python3.12/dist-packages/flash_attn-2.7.4.post1-py3.12-linux-x86_64.egg/flash_attn/ops/fused_dense.py:70: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/usr/local/lib/python3.12/dist-packages/flash_attn-2.7.4.post1-py3.12-linux-x86_64.egg/flash_attn/ops/fused_dense.py:251: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/usr/local/lib/python3.12/dist-packages/flash_attn-2.7.4.post1-py3.12-linux-x86_64.egg/flash_attn/ops/fused_dense.py:348: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "# Transformer Layer\n",
    "from flash_attn.modules.mha import MHA\n",
    "from flash_attn.ops.rms_norm import RMSNorm\n",
    "from flash_attn.ops.fused_dense import FusedMLP\n",
    "from torch import nn, optim, Tensor\n",
    "import torch.nn.functional as F\n",
    "import lightning as pl\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, layer_idx, embed_dim, num_heads, mlp_ratio=4.0, proj_groups=1,\n",
    "                 dropout=0.05, fast_attention=True):\n",
    "        super().__init__()\n",
    "        self.attn = MHA(embed_dim, num_heads, causal=True, layer_idx=layer_idx,\n",
    "                        num_heads_kv=num_heads//proj_groups,\n",
    "                        rotary_emb_dim=embed_dim//num_heads,\n",
    "                        use_flash_attn=fast_attention,\n",
    "                        return_residual=False, dropout=dropout)\n",
    "        self.norm1 = RMSNorm(embed_dim)\n",
    "        self.mlp   = FusedMLP(embed_dim, int(embed_dim*mlp_ratio))\n",
    "        self.norm2 = RMSNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a145c1",
   "metadata": {},
   "source": [
    "### B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af46f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveTransformerModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim, embed_dim, num_heads, num_layers,\n",
    "            mlp_ratio=4.0, lr=5e-5, weight_decay=1e-2\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Linear projection layer for input embedding\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Create transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(i, embed_dim, num_heads, mlp_ratio)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final linear layer for output projection\n",
    "        self.fc_out = nn.Linear(embed_dim, input_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.input_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "\n",
    "        # Provide a small weight for the volume feature\n",
    "        fw = torch.tensor([1.,1.,1.,1.,0.25])\n",
    "        self.register_buffer('feature_weights', fw)\n",
    "\n",
    "        # For tracking metrics\n",
    "        self.train_loss = 0.0\n",
    "        self.val_loss = 0.0\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # For storing test predictions\n",
    "        self.test_predictions = []\n",
    "        self.test_targets = []\n",
    "        self.test_masks = []\n",
    "        self.feature_names = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.input_proj(torch.nan_to_num(x, nan=0.0))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "    \n",
    "    def _calculate_autoregressive_loss(self, preds, targets, mask):\n",
    "        # shift for next-step prediction\n",
    "        p, t = preds[:, :-1], targets[:, 1:]\n",
    "        m = mask[:, 1:].unsqueeze(-1)\n",
    "        # mse = F.mse_loss(p, t, reduction='none')\n",
    "        mse = F.smooth_l1_loss(p, t, reduction='none', beta=0.01)\n",
    "        # apply per-feature weights\n",
    "        mse = mse * self.feature_weights.view(1,1,-1)\n",
    "        # mask and reduce\n",
    "        mse = mse * m\n",
    "        denom = m.sum().clamp_min(1.0)\n",
    "        return mse.sum() / denom\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step for autoregressive prediction\n",
    "        \"\"\"\n",
    "        # Get inputs and mask from batch\n",
    "        inputs = batch['inputs']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        # Safety check for NaN in inputs\n",
    "        if torch.isnan(inputs).any():\n",
    "            inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "        \n",
    "        # Forward pass to get predictions\n",
    "        predictions = self(inputs, mask)\n",
    "        \n",
    "        # Calculate autoregressive loss\n",
    "        loss = self._calculate_autoregressive_loss(predictions, inputs, mask)        \n",
    "        \n",
    "        # Log the loss for monitoring (don't try to compute gradient norm here)\n",
    "        self.train_loss = loss\n",
    "        self.log('train_loss', loss, prog_bar=True)  \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_after_backward(self):\n",
    "        \"\"\"\n",
    "        Called after .backward() and before optimizers do anything.\n",
    "        This is the right place to check gradients.\n",
    "        \"\"\"\n",
    "        # Safely compute gradient norm - after backward pass when gradients exist\n",
    "        if any(p.grad is not None for p in self.parameters()):\n",
    "            grad_list = [p.grad.detach().norm(2) for p in self.parameters() if p.grad is not None]\n",
    "            if grad_list:  # Make sure list is not empty\n",
    "                grad_norm = torch.stack(grad_list).norm(2)\n",
    "                self.log('grad_norm', grad_norm, prog_bar=True)\n",
    "            else:\n",
    "                self.log('grad_norm', torch.tensor(0.0), prog_bar=True)\n",
    "        else:\n",
    "            self.log('grad_norm', torch.tensor(0.0), prog_bar=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step for autoregressive prediction\n",
    "        \"\"\"\n",
    "        # Get inputs and mask from batch\n",
    "        inputs = batch['inputs']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        # Safety check for NaN in inputs\n",
    "        if torch.isnan(inputs).any():\n",
    "            inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "        \n",
    "        # Forward pass to get predictions\n",
    "        predictions = self(inputs, mask)\n",
    "        \n",
    "        # Calculate autoregressive loss\n",
    "        loss = self._calculate_autoregressive_loss(predictions, inputs, mask)\n",
    "                \n",
    "        # Log the loss for monitoring\n",
    "        self.val_loss = loss\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step for evaluating the model after training\n",
    "        \"\"\"\n",
    "        # Get inputs and mask from batch\n",
    "        inputs = batch['inputs']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        # Safety check for NaN in inputs\n",
    "        if torch.isnan(inputs).any():\n",
    "            inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "        \n",
    "        # Forward pass to get predictions\n",
    "        predictions = self(inputs, mask)\n",
    "        \n",
    "        # Calculate autoregressive loss\n",
    "        loss = self._calculate_autoregressive_loss(predictions, inputs, mask)\n",
    "        \n",
    "        # Store predictions and targets for later analysis\n",
    "        # Use detach() and cpu() to avoid memory leaks\n",
    "        self.test_predictions.append(predictions.detach().cpu())\n",
    "        self.test_targets.append(inputs.detach().cpu())\n",
    "        self.test_masks.append(mask.detach().cpu())\n",
    "        \n",
    "        # Log the test loss\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    # Configuring the optimizer\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": opt,\n",
    "            \"lr_scheduler_config\": {\n",
    "                \"scheduler\": sch,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Calculate and log test metrics at the end of the test epoch\n",
    "        \"\"\"\n",
    "        # Concatenate all batches\n",
    "        all_preds = torch.cat(self.test_predictions, dim=0)\n",
    "        all_targets = torch.cat(self.test_targets, dim=0)\n",
    "        all_masks = torch.cat(self.test_masks, dim=0)\n",
    "        \n",
    "        # For autoregressive prediction, shift by one step\n",
    "        pred_shifted = all_preds[:, :-1, :]\n",
    "        target_shifted = all_targets[:, 1:, :]\n",
    "        mask_shifted = all_masks[:, 1:]\n",
    "        \n",
    "        # Convert to numpy for sklearn metrics\n",
    "        pred_np   = pred_shifted.detach().cpu().to(torch.float32).numpy()\n",
    "        target_np = target_shifted.detach().cpu().to(torch.float32).numpy()\n",
    "        mask_np   = mask_shifted.detach().cpu().to(torch.float32).numpy()\n",
    "        \n",
    "        # Calculate metrics for each feature\n",
    "        metrics = {}\n",
    "        for i, feature_name in enumerate(self.feature_names):\n",
    "            # Extract predictions and targets for this feature\n",
    "            feature_preds = pred_np[:, :, i]\n",
    "            feature_targets = target_np[:, :, i]\n",
    "            \n",
    "            # Apply mask to consider only valid positions\n",
    "            valid_preds = []\n",
    "            valid_targets = []\n",
    "            \n",
    "            # Flatten and filter by mask\n",
    "            for batch_idx in range(feature_preds.shape[0]):\n",
    "                for seq_idx in range(feature_preds.shape[1]):\n",
    "                    if mask_np[batch_idx, seq_idx]:\n",
    "                        valid_preds.append(feature_preds[batch_idx, seq_idx])\n",
    "                        valid_targets.append(feature_targets[batch_idx, seq_idx])\n",
    "            \n",
    "            # Convert to numpy arrays\n",
    "            valid_preds = np.array(valid_preds)\n",
    "            valid_targets = np.array(valid_targets)\n",
    "            \n",
    "            if len(valid_preds) > 0:\n",
    "                # Calculate metrics\n",
    "                mse = mean_squared_error(valid_targets, valid_preds)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(valid_targets, valid_preds)\n",
    "                \n",
    "                # Calculate MAPE (Mean Absolute Percentage Error) with handling for zeros\n",
    "                with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                    mape = np.mean(np.abs((valid_targets - valid_preds) / np.maximum(np.abs(valid_targets), 1e-8))) * 100\n",
    "                    mape = np.nan_to_num(mape, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                \n",
    "                # R-squared\n",
    "                r2 = r2_score(valid_targets, valid_preds)\n",
    "                \n",
    "                # Store metrics\n",
    "                metrics[f\"{feature_name}_mse\"] = mse\n",
    "                metrics[f\"{feature_name}_rmse\"] = rmse\n",
    "                metrics[f\"{feature_name}_mae\"] = mae\n",
    "                metrics[f\"{feature_name}_mape\"] = mape\n",
    "                metrics[f\"{feature_name}_r2\"] = r2\n",
    "                \n",
    "                # Log each metric\n",
    "                self.log(f\"test_{feature_name}_mse\", mse)\n",
    "                self.log(f\"test_{feature_name}_rmse\", rmse)\n",
    "                self.log(f\"test_{feature_name}_mae\", mae)\n",
    "                self.log(f\"test_{feature_name}_mape\", mape)\n",
    "                self.log(f\"test_{feature_name}_r2\", r2)\n",
    "        \n",
    "        # Calculate average metrics across all features\n",
    "        avg_mse = np.mean([metrics[f\"{feature}_mse\"] for feature in self.feature_names if f\"{feature}_mse\" in metrics])\n",
    "        avg_rmse = np.mean([metrics[f\"{feature}_rmse\"] for feature in self.feature_names if f\"{feature}_rmse\" in metrics])\n",
    "        avg_mae = np.mean([metrics[f\"{feature}_mae\"] for feature in self.feature_names if f\"{feature}_mae\" in metrics])\n",
    "        avg_mape = np.mean([metrics[f\"{feature}_mape\"] for feature in self.feature_names if f\"{feature}_mape\" in metrics])\n",
    "        avg_r2 = np.mean([metrics[f\"{feature}_r2\"] for feature in self.feature_names if f\"{feature}_r2\" in metrics])\n",
    "        \n",
    "        # Log average metrics\n",
    "        self.log(\"test_avg_mse\", avg_mse)\n",
    "        self.log(\"test_avg_rmse\", avg_rmse)\n",
    "        self.log(\"test_avg_mae\", avg_mae)\n",
    "        self.log(\"test_avg_mape\", avg_mape)\n",
    "        self.log(\"test_avg_r2\", avg_r2)\n",
    "        \n",
    "        # Print summary of test metrics\n",
    "        print(\"\\n===== TEST METRICS =====\")\n",
    "        print(f\"Average MSE: {avg_mse:.6f}\")\n",
    "        print(f\"Average RMSE: {avg_rmse:.6f}\")\n",
    "        print(f\"Average MAE: {avg_mae:.6f}\")\n",
    "        print(f\"Average MAPE: {avg_mape:.6f}%\")\n",
    "        print(f\"Average R²: {avg_r2:.6f}\")\n",
    "        print(\"=======================\\n\")\n",
    "        \n",
    "        # Create and save visualizations\n",
    "        self._create_prediction_visualizations(pred_shifted, target_shifted, mask_shifted)\n",
    "        \n",
    "        # Clear stored predictions to free memory\n",
    "        self.test_predictions = []\n",
    "        self.test_targets = []\n",
    "        self.test_masks = []\n",
    "\n",
    "    def _create_prediction_visualizations(self, predictions, targets, masks, num_samples=3):\n",
    "        \"\"\"\n",
    "        Create and save visualization of predictions vs targets\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions [batch_size, seq_len, feature_dim]\n",
    "            targets: Target values [batch_size, seq_len, feature_dim]\n",
    "            masks: Boolean masks [batch_size, seq_len]\n",
    "            num_samples: Number of samples to visualize\n",
    "        \"\"\"\n",
    "        # Convert to numpy for plotting\n",
    "        preds_np = predictions.detach().cpu().to(torch.float32).numpy()\n",
    "        targets_np = targets.detach().cpu().to(torch.float32).numpy()\n",
    "        masks_np = masks.detach().cpu().to(torch.float32).numpy()\n",
    "        \n",
    "        # Create directory for plots if it doesn't exist\n",
    "        import os\n",
    "        os.makedirs(os.path.join(CKPT_DIR, \"plots\"), exist_ok=True)\n",
    "        \n",
    "        # Plot for each feature\n",
    "        for feature_idx, feature_name in enumerate(self.feature_names):\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            \n",
    "            # Plot for a few random samples\n",
    "            for sample_idx in range(min(num_samples, preds_np.shape[0])):\n",
    "                # Get predictions and targets for this sample and feature\n",
    "                sample_preds = preds_np[sample_idx, :, feature_idx]\n",
    "                sample_targets = targets_np[sample_idx, :, feature_idx]\n",
    "                sample_mask = masks_np[sample_idx, :]\n",
    "                \n",
    "                # Create time index for x-axis\n",
    "                time_idx = np.arange(len(sample_preds))\n",
    "                \n",
    "                # Plot targets\n",
    "                plt.subplot(num_samples, 1, sample_idx + 1)\n",
    "                plt.plot(time_idx, sample_targets, 'b-', label='Actual', alpha=0.7)\n",
    "                \n",
    "                # Plot predictions (only where mask is True)\n",
    "                masked_preds = np.where(sample_mask, sample_preds, np.nan)\n",
    "                plt.plot(time_idx, masked_preds, 'r-', label='Predicted', alpha=0.7)\n",
    "                \n",
    "                # Add title and legend\n",
    "                plt.title(f\"Sample {sample_idx+1}: {feature_name}\")\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Calculate metrics for this sample\n",
    "                valid_indices = np.where(sample_mask)[0]\n",
    "                if len(valid_indices) > 0:\n",
    "                    valid_preds = sample_preds[valid_indices]\n",
    "                    valid_targets = sample_targets[valid_indices]\n",
    "                    \n",
    "                    mse = mean_squared_error(valid_targets, valid_preds)\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    r2 = r2_score(valid_targets, valid_preds)\n",
    "                    \n",
    "                    plt.figtext(0.01, 0.5 - 0.15 * sample_idx, \n",
    "                                f\"MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\", \n",
    "                                fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(CKPT_DIR, \"plots\", f\"{feature_name}_predictions.svg\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Create a summary plot with all features for the first sample\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        for feature_idx, feature_name in enumerate(self.feature_names):\n",
    "            plt.subplot(len(self.feature_names), 1, feature_idx + 1)\n",
    "            \n",
    "            # Get predictions and targets for first sample and this feature\n",
    "            sample_preds = preds_np[0, :, feature_idx]\n",
    "            sample_targets = targets_np[0, :, feature_idx]\n",
    "            sample_mask = masks_np[0, :]\n",
    "            \n",
    "            # Create time index for x-axis\n",
    "            time_idx = np.arange(len(sample_preds))\n",
    "            \n",
    "            # Plot targets\n",
    "            plt.plot(time_idx, sample_targets, 'b-', label='Actual', alpha=0.7)\n",
    "            \n",
    "            # Plot predictions (only where mask is True)\n",
    "            masked_preds = np.where(sample_mask, sample_preds, np.nan)\n",
    "            plt.plot(time_idx, masked_preds, 'r-', label='Predicted', alpha=0.7)\n",
    "            \n",
    "            # Add title and legend\n",
    "            plt.title(f\"{feature_name}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(CKPT_DIR, \"plots\", \"all_features_predictions.svg\"))\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057bb792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspace/datasets/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | input_proj | Linear     | 384    | train\n",
      "1 | layers     | ModuleList | 199 K  | train\n",
      "2 | fc_out     | Linear     | 325    | train\n",
      "--------------------------------------------------\n",
      "200 K     Trainable params\n",
      "0         Non-trainable params\n",
      "200 K     Total params\n",
      "0.801     Total estimated model params size (MB)\n",
      "59        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea52063df904bac86c77b88c50c9a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dd97e13be8488c9e579ffa84e1e481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ddf99eb8a548d19d36977f656af646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 25. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668b3b51c3a24df88fa39446dac3f01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557b2f519a2d4463a77c794affeaaa61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e114791bb9b4ca8aee7247b7f19b637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02f913c64ac40ac9ba273068add5f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288465e1e3c7458b8f8e3bda56db91f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31c9e764629407a87a5a467210bbe41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90974b3478a9439eaf51631f73722702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9617fda312a2487d86896f0b574ef673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c0162b5a7b4c53becf9e5b6e413dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "# Creating a dict of parameters for the model for better readability\n",
    "# and to be ready for cloud training\n",
    "# Note: The parameters below are just examples. You may need to adjust them based on your specific use case.\n",
    "parameters = dict(\n",
    "    input_dim = 5,\n",
    "    embed_dim = 64,\n",
    "    num_heads = 8,\n",
    "    num_layers = 4,\n",
    "    lr = 5e-5,                      # Reduced learning rate\n",
    "    weight_decay = 1e-3,            # Updated weight decay\n",
    "    limit_val_batches = 100,        # Use a fraction of validation data for faster training\n",
    "    accumulate_grad_batches = 8,    # Gradient accumulation\n",
    "    gradient_clip_val = 0.5,        # Gradient clipping for stability\n",
    "    batch_size = 32,                # Batch size\n",
    "    sequence_length = 2048,         # Sequence length\n",
    "    num_workers = 4,                # Number of workers for data loading\n",
    "    cktp_dir = CKPT_DIR,            # Checkpoint directory\n",
    "    nodes = 1,                      # Number of nodes\n",
    "    devices = 1,                    # Number of devices (GPUs)\n",
    "    accelerator = \"gpu\",            # Use GPU for training\n",
    "    strategy = \"auto\",              # Distributed Data Parallel\n",
    "    precision = \"bf16-mixed\",       # Mixed precision training\n",
    "    epochs = 10,                    # Number of epochs\n",
    "    log_every_n_steps = 10,         # Log every n steps\n",
    "    data_dir = PROCESSED_DATA_DIR,  # Directory for processed data\n",
    "    data_splits = [0.8, 0.1, 0.1],  # Train/Validation/Test splits\n",
    ")\n",
    "\n",
    "model = AutoregressiveTransformerModel(**parameters)\n",
    "\n",
    "# Lightning callbacks for early stopping and model checkpointing\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=parameters['cktp_dir'],\n",
    "    filename='best-model-{epoch:02d}-{val_loss:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    limit_val_batches=parameters['limit_val_batches'],\n",
    "    max_epochs=parameters['epochs'], \n",
    "    accumulate_grad_batches=parameters['accumulate_grad_batches'], \n",
    "    gradient_clip_val=parameters['gradient_clip_val'],  \n",
    "    default_root_dir=parameters['cktp_dir'],\n",
    "    precision=parameters['precision'],\n",
    "    log_every_n_steps=parameters['log_every_n_steps'],\n",
    "    accelerator=parameters['accelerator'],\n",
    "    devices=parameters['devices'],\n",
    "    strategy=parameters['strategy'],\n",
    "    num_nodes=parameters['nodes'],\n",
    "    callbacks=[early_stop_callback, checkpoint_callback]\n",
    ") \n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c81720",
   "metadata": {},
   "source": [
    "Now that we have trained the model, lets look at the training characteristics we caputured during the training process with tensorboard. Below is the code to read and render a image from the tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tensorboard_scalars(ckpt_dir:str, tag=None, tags=None):\n",
    "    \"\"\"\n",
    "    Plot scalar curves from TensorBoard logs in a given directory.\n",
    "\n",
    "    Args:\n",
    "        tag (str): Specific tag to plot.\n",
    "        tags (list): List of tags to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "    \n",
    "    # Find the most recent version\n",
    "    version_dirs = glob.glob(f\"{ckpt_dir}/lightning_logs/version_*\")\n",
    "    if not version_dirs:\n",
    "        print(\"No training logs found.\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    latest_version = max(version_dirs, key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "    # Find any event files in the directory\n",
    "    files = glob.glob(os.path.join(latest_version, 'events.out.tfevents.*'))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No TensorBoard event files found in {latest_version}\")\n",
    "\n",
    "    # Initialize the EventAccumulator to read scalars\n",
    "    ea = event_accumulator.EventAccumulator(\n",
    "        latest_version,\n",
    "        size_guidance={  # Load all scalar data\n",
    "            event_accumulator.SCALARS: 0,\n",
    "        }\n",
    "    )\n",
    "    ea.Reload()  # Load the event data\n",
    "\n",
    "    # Determine which tags to plot\n",
    "    available_tags = ea.Tags().get('scalars', [])\n",
    "    if tags:\n",
    "        plot_tags = [t for t in tags if t in available_tags]\n",
    "    elif tag:\n",
    "        if tag not in available_tags:\n",
    "            raise ValueError(f\"Tag '{tag}' not found. Available tags: {available_tags}\")\n",
    "        plot_tags = [tag]\n",
    "    else:\n",
    "        # If no tag(s) specified, plot all scalar tags\n",
    "        plot_tags = available_tags\n",
    "\n",
    "    if not plot_tags:\n",
    "        raise ValueError(\"No valid scalar tags to plot.\")\n",
    "\n",
    "    # Plot each tag's values over steps\n",
    "    plt.figure()\n",
    "    for t in plot_tags:\n",
    "        events = ea.Scalars(t)\n",
    "        steps = [e.step for e in events]\n",
    "        values = [e.value for e in events]\n",
    "        plt.plot(steps, values, label=t)\n",
    "\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f\"TensorBoard Scalars\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ckpt_dir, \"plots\", \"tensorBoard_scalars.svg\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plot_tensorboard_scalars(ckpt_dir=parameters['data_dir'], tags=['train_loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f75ad6",
   "metadata": {},
   "source": [
    "Below is the scalar plot of the training and validation loss over the epochs. The training loss is shown in blue and the validation loss is shown in orange. As we can see, the training loss decreases over time, indicating that the model is learning. The validation loss also decreases, but at a slower rate, indicating that the model is not overfitting to the training data.\n",
    "\n",
    "![Flash Attention Predictions](../assets/images/flash_attn_tensorBoard_scalars.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c720ad",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "\n",
    "### Test Dataset\n",
    "Now that we have trained the model, we can evaluate it on the test dataset. We will use the `evaluate` method from the `litdata` library to evaluate the model on the test dataset. Below is the code to evaluate the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3761be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from /workspace/datasets/checkpoints/best-model-epoch=08-val_loss=0.2039.ckpt\n",
      "Evaluating model on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788e507566b64bd2aaf87b4a9dcdfc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TEST METRICS =====\n",
      "Average MSE: 0.254523\n",
      "Average RMSE: 0.247193\n",
      "Average MAE: 0.172371\n",
      "Average MAPE: 80.817858%\n",
      "Average R²: 0.824211\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Close_mae       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.01602933742105961    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Close_mape      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.417116165161133     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Close_mse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0006358284736052155   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Close_r2       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9900671243667603     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Close_rmse      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.025215638801455498    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_High_mae       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.01634710468351841    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_High_mape       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.466440439224243     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_High_mse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0005231107934378088   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_High_r2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.991832971572876     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_High_rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.02287161536514759    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Low_mae        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.022319288924336433    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Low_mape       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.4797444343566895     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Low_mse        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0012872182996943593   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_Low_r2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9798782467842102     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Low_rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03587782382965088    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Open_mae       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.016561882570385933    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Open_mape       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.5399210453033447     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Open_mse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.0006383138825185597   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_Open_r2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9900282025337219     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Open_rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.02526487410068512    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Volume_mae      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7905952334403992     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_Volume_mape      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     393.1860656738281     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Volume_mse      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2695327997207642     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_Volume_r2       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16924923658370972    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_Volume_rmse      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1267354488372803     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_avg_mae        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1723705679178238     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_avg_mape       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     80.81785583496094     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_avg_mse        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.25452345609664917    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_avg_r2        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8242111802101135     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_avg_rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2471930831670761     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2503935396671295     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Close_mae      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.01602933742105961   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Close_mape     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.417116165161133    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Close_mse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0006358284736052155  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Close_r2      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9900671243667603    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Close_rmse     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.025215638801455498   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_High_mae      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.01634710468351841   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_High_mape      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.466440439224243    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_High_mse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0005231107934378088  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_High_r2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.991832971572876    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_High_rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.02287161536514759   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Low_mae       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.022319288924336433   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Low_mape      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.4797444343566895    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Low_mse       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0012872182996943593  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_Low_r2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9798782467842102    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Low_rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03587782382965088   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Open_mae      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.016561882570385933   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Open_mape      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.5399210453033447    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Open_mse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.0006383138825185597  \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_Open_r2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9900282025337219    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Open_rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.02526487410068512   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Volume_mae     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7905952334403992    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_Volume_mape     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    393.1860656738281    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Volume_mse     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2695327997207642    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_Volume_r2      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16924923658370972   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test_Volume_rmse     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1267354488372803    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_avg_mae       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1723705679178238    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_avg_mape      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    80.81785583496094    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_avg_mse       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.25452345609664917   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_avg_r2       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8242111802101135    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_avg_rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2471930831670761    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2503935396671295    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.2503935396671295,\n",
       "  'test_Open_mse': 0.0006383138825185597,\n",
       "  'test_Open_rmse': 0.02526487410068512,\n",
       "  'test_Open_mae': 0.016561882570385933,\n",
       "  'test_Open_mape': 2.5399210453033447,\n",
       "  'test_Open_r2': 0.9900282025337219,\n",
       "  'test_High_mse': 0.0005231107934378088,\n",
       "  'test_High_rmse': 0.02287161536514759,\n",
       "  'test_High_mae': 0.01634710468351841,\n",
       "  'test_High_mape': 2.466440439224243,\n",
       "  'test_High_r2': 0.991832971572876,\n",
       "  'test_Low_mse': 0.0012872182996943593,\n",
       "  'test_Low_rmse': 0.03587782382965088,\n",
       "  'test_Low_mae': 0.022319288924336433,\n",
       "  'test_Low_mape': 3.4797444343566895,\n",
       "  'test_Low_r2': 0.9798782467842102,\n",
       "  'test_Close_mse': 0.0006358284736052155,\n",
       "  'test_Close_rmse': 0.025215638801455498,\n",
       "  'test_Close_mae': 0.01602933742105961,\n",
       "  'test_Close_mape': 2.417116165161133,\n",
       "  'test_Close_r2': 0.9900671243667603,\n",
       "  'test_Volume_mse': 1.2695327997207642,\n",
       "  'test_Volume_rmse': 1.1267354488372803,\n",
       "  'test_Volume_mae': 0.7905952334403992,\n",
       "  'test_Volume_mape': 393.1860656738281,\n",
       "  'test_Volume_r2': 0.16924923658370972,\n",
       "  'test_avg_mse': 0.25452345609664917,\n",
       "  'test_avg_rmse': 0.2471930831670761,\n",
       "  'test_avg_mae': 0.1723705679178238,\n",
       "  'test_avg_mape': 80.81785583496094,\n",
       "  'test_avg_r2': 0.8242111802101135}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "if best_model_path:\n",
    "    print(f\"Loading best model from {best_model_path}\")\n",
    "    model = AutoregressiveTransformerModel.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Test the model and calculate metrics\n",
    "print(\"Evaluating model on test set...\")\n",
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713e773",
   "metadata": {},
   "source": [
    "Now lets go ahead and create some figures to better visualize the results. We will create a figure with a subplots for each of the predictions. Each subplot will plot the difference between the actual values and the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d2713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metrics file found at /workspace/datasets/checkpoints/lightning_logs/version_0/metrics.csv\n",
      "Test performance report saved to /workspace/datasets/checkpoints/test_performance_report.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# Create a performance report with all the metrics\n",
    "def create_performance_report(ckpt_dir: str = CKPT_DIR):\n",
    "    # Get metrics from the lightning logs\n",
    "    import json\n",
    "    import glob\n",
    "    \n",
    "    # Find the most recent version\n",
    "    version_dirs = glob.glob(f\"{ckpt_dir}/lightning_logs/version_*\")\n",
    "    if not version_dirs:\n",
    "        print(\"No training logs found.\")\n",
    "        return\n",
    "    \n",
    "    latest_version = max(version_dirs, key=lambda x: int(x.split('_')[-1]))\n",
    "    metrics_path = f\"{latest_version}/metrics.csv\"\n",
    "    \n",
    "    if not os.path.exists(metrics_path):\n",
    "        print(f\"No metrics file found at {metrics_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load metrics\n",
    "    metrics_df = pd.read_csv(metrics_path)\n",
    "    \n",
    "    # Filter for test metrics only\n",
    "    test_metrics = metrics_df[metrics_df['step'].isna()]\n",
    "    \n",
    "    # Create a report\n",
    "    with open(f\"{ckpt_dir}/test_performance_report.md\", 'w') as f:\n",
    "        f.write(\"# Model Performance Report\\n\\n\")\n",
    "        f.write(\"## Overall Metrics\\n\\n\")\n",
    "        \n",
    "        # Add overall metrics\n",
    "        if 'test_avg_mse' in test_metrics.columns:\n",
    "            f.write(f\"* **Average MSE:** {test_metrics['test_avg_mse'].iloc[-1]:.6f}\\n\")\n",
    "        if 'test_avg_rmse' in test_metrics.columns:\n",
    "            f.write(f\"* **Average RMSE:** {test_metrics['test_avg_rmse'].iloc[-1]:.6f}\\n\")\n",
    "        if 'test_avg_mae' in test_metrics.columns:\n",
    "            f.write(f\"* **Average MAE:** {test_metrics['test_avg_mae'].iloc[-1]:.6f}\\n\")\n",
    "        if 'test_avg_mape' in test_metrics.columns:\n",
    "            f.write(f\"* **Average MAPE:** {test_metrics['test_avg_mape'].iloc[-1]:.6f}%\\n\")\n",
    "        if 'test_avg_r2' in test_metrics.columns:\n",
    "            f.write(f\"* **Average R²:** {test_metrics['test_avg_r2'].iloc[-1]:.6f}\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Feature-Specific Metrics\\n\\n\")\n",
    "        \n",
    "        # Add feature-specific metrics\n",
    "        for feature in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "            f.write(f\"### {feature}\\n\\n\")\n",
    "            \n",
    "            if f'test_{feature}_mse' in test_metrics.columns:\n",
    "                f.write(f\"* **MSE:** {test_metrics[f'test_{feature}_mse'].iloc[-1]:.6f}\\n\")\n",
    "            if f'test_{feature}_rmse' in test_metrics.columns:\n",
    "                f.write(f\"* **RMSE:** {test_metrics[f'test_{feature}_rmse'].iloc[-1]:.6f}\\n\")\n",
    "            if f'test_{feature}_mae' in test_metrics.columns:\n",
    "                f.write(f\"* **MAE:** {test_metrics[f'test_{feature}_mae'].iloc[-1]:.6f}\\n\")\n",
    "            if f'test_{feature}_mape' in test_metrics.columns:\n",
    "                f.write(f\"* **MAPE:** {test_metrics[f'test_{feature}_mape'].iloc[-1]:.6f}%\\n\")\n",
    "            if f'test_{feature}_r2' in test_metrics.columns:\n",
    "                f.write(f\"* **R²:** {test_metrics[f'test_{feature}_r2'].iloc[-1]:.6f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Add prediction plots\n",
    "        f.write(\"## Prediction Visualizations\\n\\n\")\n",
    "        for feature in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "            f.write(f\"### {feature} Predictions\\n\\n\")\n",
    "            f.write(f\"![{feature} Predictions](plots/{feature}_predictions.svg)\\n\\n\")\n",
    "        \n",
    "        f.write(\"### All Features (Sample 1)\\n\\n\")\n",
    "        f.write(\"![All Features](plots/all_features_predictions.svg)\\n\\n\")\n",
    "\n",
    "# Create the performance report\n",
    "create_performance_report(ckpt_dir=parameters['ckpt_dir'])\n",
    "print(f\"Test performance report saved to {parameters['ckpt_dir']}/test_performance_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc37cd9",
   "metadata": {},
   "source": [
    "![Flash Attention Predictions](../assets/images/flash_attn_all_features_predictions.svg)\n",
    "\n",
    "Let's take a look at how the model performs differently across features. The metrics table shows:\n",
    "\n",
    "| Feature | FlashAttention R²  | Flash Attention MAPE |\n",
    "| ------- | ------------------ | -------------------- |\n",
    "| Close   | 0.82               | 2.41                 |\n",
    "| Open    | 0.81               | 2.54                 |\n",
    "| High    | 0.77               | 2.46                 |\n",
    "| Low     | 0.77               | 2.47                 |\n",
    "| Volume  | 0.16               | 393.19               |\n",
    "\n",
    "Lets first start with the the high R² values. The model performs well on the close, open, high, and low price predictions, with R² values ranging from 0.77 to 0.82. This indicates that the model is able to explain a significant portion of the variance in the data. The mean absolute percentage error (MAPE) values for these features are also relatively low, ranging from 0.8% to 1.2%. This indicates that the model is able to make accurate predictions for these features. This is a good sign, as it indicates that the model is able to learn the underlying patterns in the data and make accurate predictions. However, this is likely due to the fact that these features are highly correlated with each other and the model is able to learn these correlations. Additionally, the variance in the data is relatively low, which makes it easier for the model to learn the underlying patterns.\n",
    "\n",
    "Volume is clearly the most challenging feature to predict since it has the highest variance in the dataset. With an R² value of only 0.16 and extremely high MAPE of 253% the model struggles to make accurate volume predictions. The high MAPE value indicates substantial percentage errors in predictions. This happens when:\n",
    "\n",
    "- Working with normalized data where values are close to zero\n",
    "- Dealing with highly volatile series like cryptocurrency prices\n",
    "- The model struggles with abrupt changes\n",
    "\n",
    "We can improve the model in many ways, such as:\n",
    "- Data transformation: Better normalization or scaling\n",
    "- Feature engineering: Adding more features or using different features such as technical indicators or more complex features such as wavelet transforms.\n",
    "- Model architecture: Using more complex architectures such as CNNs or LSTMs or alternative attention architectures such as state space models.\n",
    "- Hyperparameter tuning: Tuning the hyperparameters of the model to improve performance\n",
    "- Regularization: Adding regularization to the model to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2692f",
   "metadata": {},
   "source": [
    "## Scaling with GCP Cloud Computing\n",
    "\n",
    "This section demonstrates how to scale our machine learning model training to Google Cloud Platform (GCP) using Vertex AI. Vertex AI is a managed machine learning platform that enables you to train and deploy ML models at scale with comprehensive tooling and infrastructure.\n",
    "\n",
    "### GCP Setup Prerequisites\n",
    "1. Create a GCP project and enable the Vertex AI API\n",
    "2. Create a GCP service account with appropriate permissions\n",
    "3. Download the JSON key file for authentication\n",
    "4. Set the environment variable `GOOGLE_APPLICATION_CREDENTIALS` to the path of the JSON key file\n",
    "\n",
    "Steps 1-3 are covered in the [GCP setup](#gcp-setup) section. Below, we'll focus on steps 4-6:\n",
    "- Building and pushing Docker images to GCP Artifact Registry\n",
    "- Uploading configuration, training scripts, and datasets to GCP Cloud Storage\n",
    "- Submitting training jobs to Vertex AI\n",
    "\n",
    "### Step 4: Build and Push the Docker Image\n",
    "\n",
    "First, authenticate Docker with GCP Artifact Registry:\n",
    "\n",
    "```bash\n",
    "# Configure Docker to use GCP credentials for authentication\n",
    "gcloud auth configure-docker your-location-docker.pkg.dev\n",
    "```\n",
    "\n",
    "Next, build the Docker image with the same Dockerfile used for local development:\n",
    "\n",
    "```bash\n",
    "# Build the Docker image with GCP Artifact Registry URL\n",
    "docker build -f ./assets/build/Dockerfile.flashattn.cu128py26cp312 \\\n",
    "  -t your-location-docker.pkg.dev/your-project-id/repositories/flash-attention:latest .\n",
    "\n",
    "# Push the Docker image to GCP Artifact Registry\n",
    "docker push your-location-docker.pkg.dev/your-project-id/repositories/flash-attention:latest\n",
    "\n",
    "# Verify the Docker image in GCP Artifact Registry\n",
    "gcloud artifacts docker images list your-location-docker.pkg.dev/your-project-id/repositories/flash-attention\n",
    "```\n",
    "\n",
    "> **Notes:**\n",
    "> - Replace `your-location` with your GCP region (e.g., `us-central1`)\n",
    "> - Replace `your-project-id` with your GCP project ID\n",
    "> - You must first create a repository in Artifact Registry called `repositories` (or use your preferred name)\n",
    "\n",
    "### Step 5: Upload Files to GCP Cloud Storage\n",
    "\n",
    "First, create a GCP Cloud Storage bucket:\n",
    "\n",
    "```bash\n",
    "# Create a GCP Cloud Storage bucket\n",
    "gsutil mb -l your-location gs://your-gcp-training-bucket\n",
    "```\n",
    "\n",
    "Next, create the configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c09a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "GCP_DATA_DIR = \"/gcs/your-gcp-training-bucket/flash-attn-example/dataset/\"\n",
    "GCP_CKPT_DIR = \"/gcs/your-gcp-training-bucket/flash-attn-example/checkpoints/\"\n",
    "\n",
    "# Model and training parameters\n",
    "parameters = dict(\n",
    "    input_dim = 5,                   # Number of features in the input data  \n",
    "    embed_dim = 64,                  # Embedding dimension\n",
    "    num_heads = 8,                   # Number of attention heads    \n",
    "    num_layers = 4,                  # Number of transformer layers\n",
    "    lr = 5e-5,                       # Learning rate\n",
    "    weight_decay = 1e-3,             # Weight decay for regularization\n",
    "    limit_val_batches = 100,         # Fraction of validation data to use\n",
    "    accumulate_grad_batches = 8,     # Gradient accumulation steps\n",
    "    gradient_clip_val = 0.5,         # Gradient clipping value\n",
    "    batch_size = 32,                 # Batch size\n",
    "    sequence_length = 2048,          # Sequence length\n",
    "    num_workers = 4,                 # Data loading workers\n",
    "    cktp_dir = GCP_CKPT_DIR,         # Checkpoint directory\n",
    "    nodes = 1,                       # Number of nodes\n",
    "    devices = 8,                     # Number of GPUs\n",
    "    accelerator = \"gpu\",             # Accelerator type\n",
    "    strategy = \"auto\",               # Training strategy\n",
    "    precision = \"bf16-mixed\",        # Mixed precision\n",
    "    epochs = 50,                     # Training epochs\n",
    "    log_every_n_steps = 10,          # Logging frequency\n",
    "    data_dir = GCP_DATA_DIR,         # Data directory\n",
    "    data_splits = [0.8, 0.1, 0.1],   # Train/Val/Test splits\n",
    ")\n",
    "\n",
    "# Save the parameters to a YAML file\n",
    "with open(\"/workspace/datasets/flash_attn_crypto_model_config.yaml\", 'w') as f:\n",
    "    yaml.dump(parameters, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dabc09",
   "metadata": {},
   "source": [
    "Then, upload the configuration file, training script, and dataset to GCP Cloud Storage:\n",
    "\n",
    "```bash\n",
    "# Create directories in GCP Cloud Storage\n",
    "gsutil mb -p gs://your-gcp-training-bucket/flash-attn-example/config\n",
    "gsutil mb -p gs://your-gcp-training-bucket/flash-attn-example/scripts\n",
    "gsutil mb -p gs://your-gcp-training-bucket/flash-attn-example/datasets\n",
    "gsutil mb -p gs://your-gcp-training-bucket/flash-attn-example/checkpoints\n",
    "gsutil mb -p gs://your-gcp-training-bucket/flash-attn-example/staging\n",
    "\n",
    "# Upload the config file\n",
    "gsutil cp /workspace/datasets/flash_attn_crypto_model_config.yaml \\\n",
    "  gs://your-gcp-training-bucket/flash-attn-example/config/\n",
    "\n",
    "# Upload the training script\n",
    "gsutil cp /workspace/scripts/flash_attn_train.py \\\n",
    "  gs://your-gcp-training-bucket/flash-attn-example/scripts/\n",
    "\n",
    "# Upload the dataset (multi-threaded for faster upload)\n",
    "gsutil -m cp -r /workspace/datasets/auto_regressive_processed_timeseries \\\n",
    "  gs://your-gcp-training-bucket/flash-attn-example/datasets/\n",
    "\n",
    "# Verify uploads\n",
    "gsutil ls -r gs://your-gcp-training-bucket/flash-attn-example/\n",
    "```\n",
    "\n",
    "### Step 6: Submit the Training Job to Vertex AI\n",
    "\n",
    "Use the `google-cloud-aiplatform` Python library to submit the training job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "import os\n",
    "\n",
    "# Vertex AI Configuration\n",
    "SERVICE_KEY_PATH = os.getenv(\n",
    "    \"GOOGLE_APPLICATION_CREDENTIALS\", \n",
    "    \"/path/to/your/service_account_key.json\"\n",
    ")\n",
    "LOCATION = \"your-gcp-region\"         # e.g., \"us-central1\"\n",
    "ZONE = \"your-gcp-zone\"               # e.g., \"us-central1-a\"\n",
    "PROJECT_ID = \"your-gcp-project-id\"   # e.g., \"my-project-12345\"\n",
    "RESERVATION_TYPE = \"ANY\"             # or \"ANY_RESERVATION\"\n",
    "STAGING_BUCKET = \"gs://your-gcp-training-bucket/flash-attn-example/staging\"\n",
    "SERVICE_ACCOUNT = f\"vertex-ai@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "TRAIN_IMAGE = f\"your-location-docker.pkg.dev/{PROJECT_ID}/repositories/flash-attention:latest\"\n",
    "DISPLAY_NAME = \"flash-attn-crypto-model-training\"\n",
    "\n",
    "# Hardware Configuration\n",
    "NODES = 1\n",
    "MACHINE_TYPE = \"a3-megagpu-8g\"\n",
    "ACCELERATOR_TYPE = \"NVIDIA_H100_MEGA_80GB\"\n",
    "ACCELERATOR_COUNT = 8\n",
    "\n",
    "# Training Command\n",
    "CMD = [\n",
    "    \"python3\", \n",
    "    \"/gcs/your-gcp-training-bucket/flash-attn-example/scripts/flash_attn_train.py\",\n",
    "    \"--config\", \n",
    "    \"/gcs/your-gcp-training-bucket/flash-attn-example/config/flash_attn_crypto_model_config.yaml\",\n",
    "]\n",
    "\n",
    "# Worker pool specification\n",
    "worker_pool_specs=[\n",
    "    {\n",
    "        \"replica_count\": NODES,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": MACHINE_TYPE,\n",
    "            \"accelerator_type\": ACCELERATOR_TYPE,\n",
    "            \"accelerator_count\": ACCELERATOR_COUNT,\n",
    "            \"reservation_affinity\": {\n",
    "                \"reservation_affinity_type\": RESERVATION_TYPE,\n",
    "            }\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"command\": CMD\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    credentials=service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_KEY_PATH\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create and submit the training job\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=DISPLAY_NAME, \n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "job.submit(\n",
    "    service_account=SERVICE_ACCOUNT\n",
    ")\n",
    "\n",
    "# Print job details\n",
    "print(f\"Job ID: {job.resource_name}\")\n",
    "print(f\"Job state: {job.state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bacee38",
   "metadata": {},
   "source": [
    "### Using Specific Reservations\n",
    "\n",
    "If you have a specific hardware reservation, you can specify it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker pool specification with specific reservation\n",
    "worker_pool_specs=[\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"a3-megagpu-8g\",\n",
    "            \"accelerator_type\": \"NVIDIA_H100_MEGA_80GB\",\n",
    "            \"accelerator_count\": 8,\n",
    "            \"reservation_affinity\": {\n",
    "                \"reservation_affinity_type\": \"SPECIFIC_RESERVATION\",\n",
    "                \"key\": \"compute.googleapis.com/reservation-name\",\n",
    "                \"values\": [\n",
    "                    f\"projects/{PROJECT_ID}/zones/{ZONE}/reservations/your-reservation-name\",\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "            \"command\": CMD\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
